@article{Ikonomov2019,
   abstract = {This paper is on Bayesian inference for parametric statistical models that are implicitly defined by a stochastic simulator which specifies how data is generated. While exact sampling is possible, evaluating the likelihood function is typically prohibitively expensive. Approximate Bayesian Computation (ABC) is a framework to perform approximate inference in such situations. While basic ABC algorithms are widely applicable, they are notoriously slow and much research has focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has recently been proposed as an efficient and embarrassingly parallel method that leverages optimisation to accelerate the inference. In this paper, we demonstrate a previously unrecognised important failure mode of OMC: It generates strongly overconfident approximations by collapsing regions of similar or near-constant posterior density into a single point. We propose an efficient, robust generalisation of OMC that corrects this. It makes fewer assumptions, retains the main benefits of OMC, and can be performed either as part of OMC or entirely as post-processing. We demonstrate the effectiveness of the proposed Robust OMC on toy examples and tasks in inverse-graphics where we perform Bayesian inference with a complex image renderer.},
   author = {Borislav Ikonomov and Michael U. Gutmann},
   editor = {Silvia Chiappa and Roberto Calandra},
   journal = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
   pages = {2819-2829},
   publisher = {PMLR},
   title = {Robust Optimisation Monte Carlo},
   volume = {108},
   url = {http://arxiv.org/abs/1904.00670},
   year = {2019},
}
@article{Lintusaari2017,
   abstract = {Bayesian inference plays an important role in phylogenetics, evolutionary biology, and in many other branches of science. It provides a principled framework for dealing with uncertainty and quantifying how it changes in the light of new evidence. For many complex models and inference problems, however, only approximate quantitative answers are obtainable. Approximate Bayesian computation (ABC) refers to a family of algorithms for approximate inference that makes a minimal set of assumptions by only requiring that sampling from a model is possible.We explain here the fundamentals of ABC, review the classical algorithms, and highlight recent developments.},
   author = {Jarno Lintusaari and Michael U. Gutmann and Ritabrata Dutta and Samuel Kaski and Jukka Corander},
   doi = {10.1093/sysbio/syw077},
   issn = {1076836X},
   issue = {1},
   journal = {Systematic Biology},
   keywords = {ABC,Approximate Bayesian computation,Bayesian inference,Likelihood-free inference,Phylogenetics,Simulator-based models,Stochastic simulation models,Treebased models},
   month = {1},
   pages = {e66-e82},
   pmid = {28175922},
   title = {Fundamentals and recent developments in approximate Bayesian computation},
   volume = {66},
   url = {http://dx.doi.org/10.1093/sysbio/syw077},
   year = {2017},
}
@inproceedings{Chen2019,
   abstract = {Approximate Bayesian computation (ABC) is a set of techniques for Bayesian inference when the likelihood is intractable but sampling from the model is possible. This work presents a simple yet effective ABC algorithm based on the combination of two classical ABC approaches — regression ABC and sequential ABC. The key idea is that rather than learning the posterior directly, we first target another auxiliary distribution that can be learned accurately by existing methods, through which we then subsequently learn the desired posterior with the help of a Gaussian copula. During this process, the complexity of the model changes adaptively according to the data at hand. Experiments on a synthetic dataset as well as three real-world inference tasks demonstrates that the proposed method is fast, accurate, and easy to use.},
   author = {Yanzhi Chen and Michael U. Gutmann},
   editor = {Kamalika Chaudhuri and Masashi Sugiyama},
   journal = {Proceedings of Machine Learning Research},
   pages = {1584-1592},
   publisher = {PMLR},
   title = {Adaptive Gaussian Copula ABC},
   volume = {89},
   url = {http://proceedings.mlr.press/v89/chen19d.html https://proceedings.mlr.press/v89/chen19d.html},
   year = {2019},
}
@inproceedings{Meeds2015,
   abstract = {We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models. The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic. For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.},
   author = {Edward Meeds and Max Welling},
   editor = {C Cortes and N Lawrence and D Lee and M Sugiyama and R Garnett},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {2080-2088},
   publisher = {Curran Associates, Inc.},
   title = {Optimization Monte Carlo: Efficient and embarrassingly parallel likelihood-free inference},
   volume = {2015-Janua},
   url = {https://proceedings.neurips.cc/paper/2015/file/a284df1155ec3e67286080500df36a9a-Paper.pdf},
   year = {2015},
}
@article{Marin2012,
   abstract = {Approximate Bayesian Computation (ABC) methods, also known as likelihood-free techniques, have appeared in the past ten years as the most satisfactory approach to intractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions brought on the original ABC algorithm in recent years. © 2011 Springer Science+Business Media, LLC.},
   author = {Jean Michel Marin and Pierre Pudlo and Christian P. Robert and Robin J. Ryder},
   doi = {10.1007/s11222-011-9288-2},
   issn = {09603174},
   issue = {6},
   journal = {Statistics and Computing},
   keywords = {ABC methodology,Bayesian model choice,Bayesian statistics,DIYABC,Likelihood-free methods},
   pages = {1167-1180},
   title = {Approximate Bayesian computational methods},
   volume = {22},
   year = {2012},
}
@article{Gutmann2016,
   abstract = {Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.},
   author = {Michael U. Gutmann and Jukka Corander},
   issn = {15337928},
   issue = {125},
   journal = {Journal of Machine Learning Research},
   keywords = {Approximate Bayesian computation,Bayesian inference,Computational efficiency,Intractable likelihood,Latent variables},
   note = {<b>From Duplicate 2 (<i>Bayesian optimization for likelihood-free inference of simulator-based statistical models</i> - Gutmann, Michael U.; Corander, Jukka)<br/></b><br/>http://arxiv.org/abs/1501.03291},
   pages = {1-47},
   title = {Bayesian optimization for likelihood-free inference of simulator-based statistical models},
   volume = {17},
   url = {http://jmlr.org/papers/v17/15-017.html},
   year = {2016},
}
@article{scikit-learn,
   author = {F Pedregosa and G Varoquaux and A Gramfort and V Michel and B Thirion and O Grisel and M Blondel and P Prettenhofer and R Weiss and V Dubourg and J Vanderplas and A Passos and D Cournapeau and M Brucher and M Perrot and E Duchesnay},
   journal = {Journal of Machine Learning Research},
   pages = {2825-2830},
   title = {Scikit-learn: Machine Learning in \{P\}ython},
   volume = {12},
   year = {2011},
}
@inproceedings{Hermans2020,
   abstract = {Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to make use of approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model. We achieve this by learning a flexible amortized estimator which approximates the likelihood-to-evidence ratio. We demonstrate that the learned ratio estimator can be embedded in MCMC samplers to approximate likelihood-ratios between consecutive states in the Markov chain, allowing us to draw samples from the intractable posterior. Techniques are presented to improve the numerical stability and to measure the quality of an approximation. The accuracy of our approach is demonstrated on a variety of benchmarks against well-established techniques. Scientific applications in physics show its applicabilit.},
   author = {Joeri Hermans and Volodimir Begy and Gilles Louppe},
   isbn = {9781713821120},
   journal = {37th International Conference on Machine Learning, ICML 2020},
   pages = {4187-4198},
   title = {Likelihood-free MCMC with amortized approximate ratio estimators},
   volume = {PartF16814},
   year = {2020},
}
@inproceedings{Papamakarios2019,
   abstract = {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.},
   author = {George Papamakarios and David C. Sterratt and Iain Murray},
   city = {Proceedings of Machine Learning Research},
   editor = {Kamalika Chaudhuri and Masashi Sugiyama},
   journal = {AISTATS 2019 - 22nd International Conference on Artificial Intelligence and Statistics},
   pages = {837-848},
   publisher = {PMLR},
   title = {Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows},
   volume = {89},
   year = {2020},
}
@generic{R,
   abstract = {Cite: (2020).},
   author = {R Core Development Team},
   city = {Vienna, Austria},
   institution = {\proglang\{R\} Foundation for Statistical Computing},
   journal = {R Foundation for Statistical Computing},
   pages = {https://www.R-project.org},
   title = {A Language and Environment for Statistical Computing},
   volume = {2},
   url = {http://www.r-project.org},
   year = {2020},
}
@generic{1708.00707,
   author = {Jarno Lintusaari and Henri Vuollekoski and Antti Kangasrääsiö and Kusti Skytén and Marko Järvenpää and Pekka Marttinen and Michael Gutmann and Aki Vehtari and Jukka Corander and Samuel Kaski},
   title = {ELFI: Engine for Likelihood Free Inference},
   year = {2018},
}
@generic{gpy2014,
   author = {GPy},
   journal = {GPy: A gaussian process framework in phython},
   pages = {http://github.com/SheffieldML/GPy},
   title = {GPy: A gaussian process framework in python},
   url = {http://github.com/SheffieldML/GPy},
   year = {2014},
}
@article{Cranmer2020,
   abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.},
   author = {K Cranmer and J Brehmer and G Louppe},
   journal = {Proceedings of the National Academy of Sciences},
   publisher = {National Academy of Sciences},
   title = {The frontier of simulation-based inference},
   year = {2020},
}
@inproceedings{Kluyver:2016aa,
   abstract = {It is increasingly necessary for researchers in all fields to write computer code, and in order to reproduce research results, it is important that this code is published. We present Jupyter notebooks, a document format for publishing code, results and explanations in a form that is both readable and executable. We discuss various tools and use cases for notebook documents.},
   author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando Pérez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Damián Avila and Safia Abdalla and Carol Willing},
   doi = {10.3233/978-1-61499-649-1-87},
   editor = {F Loizides and B Schmidt},
   isbn = {9781614996484},
   institution = {IOS Press},
   journal = {Positioning and Power in Academic Publishing: Players, Agents and Agendas - Proceedings of the 20th International Conference on Electronic Publishing, ELPUB 2016},
   keywords = {Notebook,Reproducibility,Research code},
   pages = {87-90},
   title = {Jupyter Notebooks—a publishing format for reproducible computational workflows},
   year = {2016},
}
@book{Wood:2006,
   abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models. The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book’s R data package gamair, to enable use as a course text or for self-study.},
   author = {Simon N. Wood},
   city = {Boca Raton},
   doi = {10.1201/9781315370279},
   isbn = {9781498728348},
   journal = {Generalized Additive Models: An Introduction with R, Second Edition},
   pages = {1-476},
   publisher = {Chapman \& Hall/CRC},
   title = {Generalized additive models: An introduction with R, second edition},
   year = {2017},
}
@article{Blum2010,
   abstract = {Approximate Bayesian inference on the basis of summary statistics is well-suited to complex problems for which the likelihood is either mathematically or computationally intractable. However the methods that use rejection suffer from the curse of dimensionality when the number of summary statistics is increased. Here we propose a machine-learning approach to the estimation of the posterior density by introducing two innovations. The new method fits a nonlinear conditional heteroscedastic regression of the parameter on the summary statistics, and then adaptively improves estimation using importance sampling. The new algorithm is compared to the state-of-the-art approximate Bayesian methods, and achieves considerable reduction of the computational burden in two examples of inference in statistical genetics and in a queueing model.},
   author = {Michael Blum and Olivier Francois},
   issn = {0960-3174},
   issue = {1},
   journal = {Statistics and Computing},
   keywords = {Mathematics and Statistics},
   pages = {63-73},
   publisher = {Springer Netherlands},
   title = {\{N\}on-linear regression models for \{A\}pproximate \{B\}ayesian \{C\}omputation},
   volume = {20},
   url = {http://dx.doi.org/10.1007/s11222-009-9116-0},
   year = {2010},
}
@inproceedings{Papamakarios2016,
   abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ∈-ball around the observed data, which is only correct in the limit ∈→0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as ∈ → 0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
   author = {George Papamakarios and Iain Murray},
   editor = {D D Lee and M Sugiyama and U V Luxburg and I Guyon and R Garnett},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1036-1044},
   publisher = {Curran Associates, Inc.},
   title = {Fast e-free inference of simulation models with Bayesian conditional density estimation},
   url = {http://papers.nips.cc/paper/6084-fast-free-inference-of-simulation-models-with-bayesian-conditional-density-estimation},
   year = {2016},
}
@article{2020SciPy-NMeth,
   abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
   author = {Pauli Virtanen and Ralf Gommers and Travis E. Oliphant and Matt Haberland and Tyler Reddy and David Cournapeau and Evgeni Burovski and Pearu Peterson and Warren Weckesser and Jonathan Bright and Stéfan J. van der Walt and Matthew Brett and Joshua Wilson and K. Jarrod Millman and Nikolay Mayorov and Andrew R.J. Nelson and Eric Jones and Robert Kern and Eric Larson and C. J. Carey and İlhan Polat and Yu Feng and Eric W. Moore and Jake VanderPlas and Denis Laxalde and Josef Perktold and Robert Cimrman and Ian Henriksen and E. A. Quintero and Charles R. Harris and Anne M. Archibald and Antônio H. Ribeiro and Fabian Pedregosa and Paul van Mulbregt and Aditya Vijaykumar and Alessandro Pietro Bardelli and Alex Rothberg and Andreas Hilboll and Andreas Kloeckner and Anthony Scopatz and Antony Lee and Ariel Rokem and C. Nathan Woods and Chad Fulton and Charles Masson and Christian Häggström and Clark Fitzgerald and David A. Nicholson and David R. Hagen and Dmitrii V. Pasechnik and Emanuele Olivetti and Eric Martin and Eric Wieser and Fabrice Silva and Felix Lenders and Florian Wilhelm and G. Young and Gavin A. Price and Gert Ludwig Ingold and Gregory E. Allen and Gregory R. Lee and Hervé Audren and Irvin Probst and Jörg P. Dietrich and Jacob Silterra and James T. Webber and Janko Slavič and Joel Nothman and Johannes Buchner and Johannes Kulick and Johannes L. Schönberger and José Vinícius de Miranda Cardoso and Joscha Reimer and Joseph Harrington and Juan Luis Cano Rodríguez and Juan Nunez-Iglesias and Justin Kuczynski and Kevin Tritz and Martin Thoma and Matthew Newville and Matthias Kümmerer and Maximilian Bolingbroke and Michael Tartre and Mikhail Pak and Nathaniel J. Smith and Nikolai Nowaczyk and Nikolay Shebanov and Oleksandr Pavlyk and Per A. Brodtkorb and Perry Lee and Robert T. McGibbon and Roman Feldbauer and Sam Lewis and Sam Tygier and Scott Sievert and Sebastiano Vigna and Stefan Peterson and Surhud More and Tadeusz Pudlik and Takuya Oshima and Thomas J. Pingel and Thomas P. Robitaille and Thomas Spura and Thouis R. Jones and Tim Cera and Tim Leslie and Tiziano Zito and Tom Krauss and Utkarsh Upadhyay and Yaroslav O. Halchenko and Yoshiki Vázquez-Baeza},
   doi = {10.1038/s41592-019-0686-2},
   issn = {15487105},
   issue = {3},
   journal = {Nature Methods},
   pages = {261-272},
   pmid = {32015543},
   title = {SciPy 1.0: fundamental algorithms for scientific computing in Python},
   volume = {17},
   year = {2020},
}
@book_section{Forneron2016,
   abstract = {This paper considers properties of an optimization-based sampler for targeting the posterior distribution when the likelihood is intractable. It uses auxiliary statistics to summarize information in the data and does not directly evaluate the likelihood associated with the specified parametric model. Our reverse sampler approximates the desired posterior distribution by first solving a sequence of simulated minimum distance problems. The solutions are then reweighted by an importance ratio that depends on the prior and the volume of the Jacobian matrix. By a change of variable argument, the output consists of draws from the desired posterior distribution. Optimization always results in acceptable draws. Hence, when the minimum distance problem is not too difficult to solve, combining importance sampling with optimization can be much faster than the method of Approximate Bayesian Computation that by-passes optimization.},
   author = {Jean Jacques Forneron and Serena Ng},
   doi = {10.1108/S0731-905320160000036020},
   isbn = {978-1-78560-787-5},
   issn = {07319053},
   journal = {Advances in Econometrics},
   keywords = {Approximate Bayesian Computation,Importance sampling,Indirect inference},
   pages = {389-415},
   publisher = {Emerald Publishing Ltd},
   title = {A likelihood-free reverse sampler of the posterior distribution},
   volume = {36},
   url = {https://econpapers.repec.org/RePEc:eme:aecozz:s0731-905320160000036020},
   year = {2016},
}
@generic{hagberg2008exploring,
   abstract = {NetworkX is a Python language package for exploration and analysis of networks and network algorithms. The core package provides data structures for representing many types of networks, or graphs, including simple graphs, directed graphs, and graphs with parallel edges and self-loops. The nodes in NetworkX graphs can be any (hashable) Python object and edges can contain arbitrary data; this flexibility makes NetworkX ideal for representing networks found in many different scientific fields. In addition to the basic data structures many graph algorithms are implemented for calculating network properties and structure measures: shortest paths, betweenness centrality, clustering, and degree distribution and many more. NetworkX can read and write various graph formats for easy exchange with existing data, and provides generators for many classic graphs and popular graph models, such as the Erdos-Renyi, Small World, and Barabasi-Albert models. The ease-of-use and flexibility of the Python programming language together with connection to the SciPy tools make NetworkX a powerful tool for scientific computations. We discuss some of our recent work studying synchronization of coupled oscillators to demonstrate how NetworkX enables research in the field of computational networks.},
   author = {A A Hagberg and D A Schult and P J Swart},
   institution = {Los Alamos National Lab.(LANL), Los Alamos, NM (United States)},
   journal = {7th Python in Science Conference (SciPy 2008)},
   pages = {11-15},
   title = {Exploring network structure, dynamics, and function using NetworkX},
   year = {2008},
}
@book{Sisson2018,
   abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
   author = {Jordan J. Franks},
   doi = {10.1080/01621459.2020.1846973},
   editor = {S A Sisson and Y Fan and M A Beaumont},
   issn = {0162-1459},
   issue = {532},
   journal = {Journal of the American Statistical Association},
   pages = {2100-2101},
   publisher = {Chapman and Hall/CRC Press},
   title = {Handbook of Approximate Bayesian Computation.},
   volume = {115},
   year = {2020},
}
@article{Thomas2020,
   abstract = {We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of “closeness” is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on canonical examples and employ it to perform inference for challenging stochastic nonlinear dynamical systems and high-dimensional summary statistics.},
   author = {Owen Thomas and Ritabrata Dutta and Jukka Corander and Samuel Kaski and Michael U. Gutmann},
   doi = {10.1214/20-ba1238},
   issn = {19316690},
   issue = {1},
   journal = {Bayesian Analysis},
   keywords = {approximate Bayesian computation,density-ratio estimation; logistic regression; pr,stochastic dynamical systems,summary statistics selection,synthetic likelihood},
   title = {Likelihood-Free Inference by Ratio Estimation},
   volume = {17},
   url = {https://projecteuclid.org/euclid.ba/1599876022},
   year = {2020},
}
@book_section{Bisong2019,
   abstract = {Google Colaboratory more commonly referred to as “Google Colab” or just simply “Colab” is a research project for prototyping machine learning models on powerful hardware options such as GPUs and TPUs. It provides a serverless Jupyter notebook environment for interactive development. Google Colab is free to use like other G Suite products.},
   author = {Ekaba Bisong},
   city = {Berkeley, CA},
   doi = {10.1007/978-1-4842-4470-8_7},
   isbn = {978-1-4842-4470-8},
   journal = {Building Machine Learning and Deep Learning Models on Google Cloud Platform},
   pages = {59-64},
   publisher = {Apress},
   title = {Google Colaboratory},
   url = {https://doi.org/10.1007/978-1-4842-4470-8_7},
   year = {2019},
}
@generic{Shahriari2016,
   abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
   author = {Bobak Shahriari and Kevin Swersky and Ziyu Wang and Ryan P. Adams and Nando De Freitas},
   doi = {10.1109/JPROC.2015.2494218},
   issn = {15582256},
   issue = {1},
   journal = {Proceedings of the IEEE},
   keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
   pages = {148-175},
   title = {Taking the human out of the loop: A review of Bayesian optimization},
   volume = {104},
   year = {2016},
}
@article{Tanaka2006,
   abstract = {Tuberculosis can be studied at the population level by genotyping strains of Mycobacterium tuberculosis isolated from patients. We use an approximate Bayesian computational method in combination with a stochastic model of tuberculosis transmission and mutation of a molecular marker to estimate the net transmission rate, the doubling time, and the reproductive value of the pathogen. This method is applied to a published data set from San Francisco of tuberculosis genotypes based on the marker IS6110. The mutation rate of this marker has previously been studied, and we use those estimates to form a prior distribution of mutation rates in the inference procedure. The posterior point estimates of the key parameters of interest for these data are as follows: net transmission rate, 0.69/year [95% credibility interval (C.I.) 0.38, 1.08]; doubling time, 1.08 years (95% C.I. 0.64, 1.82); and reproductive value 3.4 (95% C.I. 1.4, 79.7). These figures suggest a rapidly spreading epidemic, consistent with observations of the resurgence of tuberculosis in the United States in the 1980s and 1990s. Copyright © 2006 by the Genetics Society of America.},
   author = {Mark M. Tanaka and Andrew R. Francis and Fabio Luciani and S. A. Sisson},
   doi = {10.1534/genetics.106.055574},
   issn = {00166731},
   issue = {3},
   journal = {Genetics},
   pages = {1511-1520},
   pmid = {16624908},
   title = {Using approximate bayesian computation to estimate tuberculosis transmission parameters from genotype data},
   volume = {173},
   year = {2006},
}
