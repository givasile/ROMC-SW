\documentclass[article]{jss}
%% -- LaTeX packages and custom commands ---------------------------------------

\usepackage[usenames,dvipsnames]{xcolor}

%% recommended packages
\usepackage{thumbpdf,lmodern}
\usepackage{orcidlink}
\usepackage{amsmath,amssymb,bbold, xfrac}

\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption} % for subfigure
\usepackage{float}

%% TikZ packages
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, backgrounds, scopes}
\usepackage{pgfplots}
\pgfplotsset{width=6.75cm, compat=newest}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}

%% for algorithms
\usepackage{algorithm}
\usepackage{algpseudocode} % algorithmicx package

%% another package (only for this demo article)
\usepackage{framed}
\usepackage{marginnote}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% Vasilis' custom commands
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\X}{\mathbf{x}}
\newcommand{\Z}{\mathbf{z}}
\newcommand{\V}{\mathbf{V}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\Thetab}{\mathbf{\Theta}}

\newcommand{\vb}{\mathbf{v}}
\newcommand{\ub}{\mathbf{u}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\xb}{\mathbf{x}}

\newcommand{\jac}{\mathbf{J}}
\newcommand{\hessian}{\mathbf{H}}
\newcommand{\thetab}{\boldsymbol{\theta}}
\newcommand{\thetabij}{\thetab_{ij}}
\newcommand{\thetabi}{\thetab_i}

\newcommand{\simulator}{g}
\newcommand{\region}{B_{d,\epsilon}}
\newcommand{\indicator}[1]{\mathbb{1}_{#1}}
\newcommand{\regioni}{B_{d,\epsilon}^i}
\newcommand{\data}{\mathbf{y_0}}
\newcommand{\accregioni}{C^i_{\epsilon}}
\newcommand{\accregionihat}{\hat{C}^i_{\epsilon}}

\newcommand{\danger}{\fontencoding{U}\fontfamily{futs}\selectfont\char 66\relax}
\newcommand{\TODO}[1]{\textcolor{red}{TODO #1}\marginpar{\textcolor{red}{\Large \danger}}}


\newcommand{\R}{\mathbb{R}}

\newcommand{\Ex}{\mathbb{E}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

%% -- Article metainformation (author, title, ...) -----------------------------

\author{Vasilis Gkolemis~\orcidlink{0000-0002-2636-0245}\\ATHENA RC \And
  Michael Gutmann~\orcidlink{0000-0002-5329-9910}\\University of Edinburgh \And
  Henri Pesonen~\orcidlink{0000-0003-4500-2926}\\University of Oslo}

\Plainauthor{Vasilis Gkolemis, Michael Gutmann, Henri Pesonen}

\title{An Extendable \proglang{Python} Implementation of Robust Optimisation Monte Carlo}
\Plaintitle{An Extendable Python Implementation of ROMC}
\Shorttitle{An Extendable ROMC implementation}

%% - \Abstract{} almost as usual
\Abstract{Performing inference in statistical models with an
  intractable likelihood is challenging, therefore, most
  likelihood-free inference (LFI) methods encounter accuracy and
  efficiency limitations. In this paper, we present the implementation
  of the LFI method Robust Optimisation Monte Carlo (ROMC) in the
  \proglang{Python} package \pkg{ELFI}. ROMC is a novel and efficient
  (highly-parallelisable) LFI framework that provides accurate
  weighted samples from the posterior. Our implementation can be used
  in two ways. First, a scientist may use it as an out-of-the-box LFI
  algorithm; we provide an easy-to-use API harmonised with the
  principles of \pkg{ELFI}, enabling effortless comparisons with the
  rest of the methods included in the package. Additionally, we have
  carefully split ROMC into isolated components for supporting
  extensibility. A researcher may experiment with novel method(s) for
  solving part(s) of ROMC without reimplementing everything from
  scratch. In both scenarios, the ROMC parts can run in a
  fully-parallelised manner, exploiting all CPU cores. We also provide
  helpful functionalities for (i) inspecting the inference process and
  (ii) evaluating the obtained samples. Finally, we test the
  robustness of our implementation on some typical LFI examples.}

%% - \Keywords{}
\Keywords{Bayesian inference, implicit models, likelihood-free, \proglang{Python}, \pkg{ELFI}}
\Plainkeywords{Bayesian inference, implicit models, likelihood-free, Python, ELFI}

%% - \Address{} of at least one author
\Address{
  Vasilis Gkolemis\\
  Information Management Systems Institute (IMSI)\\
  ATHENA Research and Innovation Center\\
  Athens, Greece\\
  E-mail: \email{vgkolemis@athenarc.gr}\\
  URL: \url{https://givasile.github.io}
}

\begin{document}

%% -- Introduction -------------------------------------------------------------
\section{Introduction} \label{sec:intro}

Simulator-based models are particularly captivating due to the
provided modeling freedom. In essence, any data generating mechanism
that can be written as a finite set of algorithmic steps can be
programmed as a simulator-based model. In these cases, it is feasible
to generate samples using the simulator but it is infeasible to
evaluate the likelihood function. The intractability of the likelihood
makes the likelihood-free inference (LFI), i.e., the approximation of
the posterior distribution without using the likelihood function,
particularly challenging.

Optimization Monte Carlo (OMC) proposed by~\citet{Meeds2015} is a
novel LFI approach for approximating the posterior distribution. The
central idea is turning the stochastic data-generating mechanism into
a set of deterministic optimization processes. Afterwards,
\citet{Forneron2016} provided a similar method under the name `reverse
sampler'. In their work,~\citet{Ikonomov2019}, located some critical
limitations of OMC, so they proposed Robust OMC (ROMC), an alternative
version of OMC with the appropriate modifications.

In this paper, we present the implementation of ROMC at the
\proglang{Python} package \pkg{ELFI} (Engine for likelihood-free
inference)~\citet{1708.00707}. As we illustrate at
Section~\ref{sec:implementation}, we have carefully designed the
implementation to ensure extensibility. ROMC is a general framework
for obtaining weighted samples from the posterior, i.e., it defines a
sequence of algorithmic steps without enforcing a specific algorithm
for solving each step. Therefore, a researcher may use ROMC as the
backbone algorithm and develop novel methods to solve each separate
step.\footnote{For being a ready-to-use
algorithm,~\citet{Ikonomov2019} proposed a default method for each
step, but this choice is by no means restrictive.} We have designed
our software for facilitating such experimentation.

To the best of our knowledge, this is the first implementation of the
ROMC inference method to a generic LFI framework. We organize the
evaluation in three steps. First, for securing that our implementation
is accurate, we test it against an artificial example with a tractable
likelihood. The artificial example also serves as a step-by-step guide
for showcasing how to use the various functionalities of our
implementation. Second, we use the second-order moving average (MA2)
example from the \pkg{ELFI} package, using as ground truth the samples
obtained with Rejection ABC~\citet{lintusaari2017}, with a very large
number of trials. Finally, we present the execution times of ROMC,
measuring the speed-up obtained by using the parallel version of the
implementation.

% \clearpage
\section{Background}

We first give a short introduction to simulator-based models, we then
focus on OMC and its robust improvement, ROMC, and we, finally,
introduce \pkg{ELFI}, the underlying package that is used for the
implementation of ROMC.

\subsection{Simulator-based models and likelihood-free inference}

An implicit or simulator-based model is a parameterized stochastic
data generating mechanism. The key characteristic of these models is
that we can sample data points, but we cannot evaluate the
likelihood. Formally, a simulator-based model is a parameterized
family of probability density functions
\(\{ p(\yb|\thetab)\}_{\thetab}\) whose closed-form is either unknown
or computationally intractable. In these scenarios, we can only a
access the simulator \( m_r(\thetab) \), i.e., a black-box mechanism
(computer code) that generates samples \(\yb\) in a stochastic manner
from a set of parameters \(\thetab\). We denote the process of
obtaining samples from the simulator with
\( m_r(\thetab) \rightarrow \yb \). It is feasible to isolate the
randomness of the simulator by introducing the nuisance random
variables denoted by \(\ub \sim p(\ub)\). Therefore, for a tuple
\((\thetab, \ub)\), the simulator turns to a deterministic mapping
\(g\), such that \(\yb=\simulator(\thetab,\ub)\). In terms of computer
code, the randomness of a random process is governed by the global
seed. Even though each software may handle the random generation
process in a different way\footnote{For example, at
  \pkg{Numpy}~\citet{harris2020array}, the pseudo-random number
  generation is based on a global state, whereas, in
  \pkg{JAX}~\citet{jax2018github} random functions consume a key that
  is passed as parameter. }, in all cases, by sampling an integer value
and setting it as the initial seed converts the simulation to a
deterministic sequence of steps.

Simulator-based models provide considerable modeling freedom; any
physical process that can be conceptualized as a computer program of
finite steps can be modeled as a simulator-based model without any
compromise. The modeling freedom allows for any amount of hidden
(unobserved) internal variables or rule-based decisions. Hence,
implicit models are often used to model physical phenomena in the
natural sciences such as, e.g., genetics, epidemiology or
neuroscience. Further background on simulator-based models and
example applications can be found in the articles
by~\citet{gutmann2016, lintusaari2017, sisson2018, cranmer2020}.

The modeling freedom of simulator-based models, however, comes at the
price of difficulties in inferring their parameters. Denoting the
observed data as \(\data\), the main difficulty is that the likelihood
function \(l(\thetab) = p(\data|\thetab)\) is generally
intractable. To better see the sources of the intractability, and to
address them, we go back to the basic characterization of the
likelihood as the (rescaled) probability of a parameter of the model
to generate data \(\yb\) that is similar to the observed data
\(\data\). More formally, the likelihood \(l(\thetab)\) equals
\begin{equation} \label{eq:likelihood}
  l(\thetab) = \lim_{\epsilon \to 0} c_\epsilon \int_{\yb \in b_{d,\epsilon}(\data)} p(\yb|\thetab)d\yb =
  \lim_{\epsilon \to 0} c_\epsilon \Pr(\simulator(\thetab, \ub) \in \region(\data)  \mid \theta)
\end{equation}
where \(c_\epsilon\) is a proportionality factor that depends on
\(\epsilon\) and \(\region(\data)\) is an \(\epsilon\) region around \(\data\)
that is defined via a distance function \(d\), i.e., \ \(\region(\data)
:= \{\yb: d(\yb, \data) \leq \epsilon \}\).

The basic characterization of the likelihood in~\eqref{eq:likelihood}
highlights two sources of intractability; the first is the computation
of the probability
\(\Pr(\simulator(\thetab,\ub) \in \region(\data))\), the second is the
limit of \(\epsilon \to 0\). Approximating the probability with
samples becomes computationally infeasible if \(\epsilon\) is too
small. Hence, a large class of inference methods work with
\(\epsilon > 0\), which leads to the approximate likelihood function
\(l_{d, \epsilon}(\thetab)\)
\begin{equation} \label{eq:approx_likelihood}
  l_{d, \epsilon}(\thetab) = \Pr(\yb \in \region(\data) \mid \theta), \quad \text{where  } \epsilon > 0.
\end{equation}

and, in turn, to the approximate posterior

\begin{equation} \label{eq:approx_posterior}
  p_{d,\epsilon}(\thetab|\data) \propto l_{d, \epsilon}(\thetab) p(\thetab).
\end{equation}

The approximation in~\eqref{eq:approx_likelihood} is by no means the
only strategy to deal with the intractability of the likelihood
function in~\eqref{eq:likelihood}. Other strategies include modeling
the (stochastic) relationship between \(\thetab\) and \(\yb\), and its
reverse, or framing likelihood-free inference as a ratio estimation
problem, see for example \citet{blum2010, Wood2006, Papamakarios2016,
  Papamakarios2019, Chen2019, Thomas2020, Hermans2020}. However, both
OMC and robust OMC, which we introduce next, are based on the
approximation in~\eqref{eq:approx_likelihood}.

\subsection{Optimization Monte Carlo (OMC)}
\input{latex_files/ch2/2-2-OMC.tex}

\subsection{Robust optimization Monte Carlo (ROMC)}
\input{latex_files/ch2/2-3-ROMC.tex}

\subsection[Engine for likelihood-free inference (ELFI)]{\pkg{Engine for likelihood-free inference (ELFI)}}
\label{subsec:ELFI}
\input{latex_files/ch2/2-4-ELFI.tex}

%% -- Design Principles -----------------------------------------------------
\section{Implementation}
\label{sec:implementation}
This section is split in two parts. We first express ROMC as an
algorithm and then we present the general implementation principles we
follow.

\subsection{Algorithmic view of ROMC}
\input{latex_files/ch3/3-0-design-principles.tex}

\subsection{General implementation principles}
\label{subsec:general_design}
\input{latex_files/ch3/3-1-implementation-principles.tex}

%% -- ROMC implementation --
\section{Implemented functionalities}
\input{latex_files/ch4/4-0-intro.tex}

\subsection{Training part}
\label{subsec:training}
\input{latex_files/ch4/4-1-training.tex}

\subsection{Inference part}
\label{subsec:inference}
\input{latex_files/ch4/4-2-inference.tex}

\subsection{Evaluation part}
\label{subsec:evaluation}
\input{latex_files/ch4/4-3-evaluation.tex}

\subsection{Extensibility} % better title
\label{subsec:extensibility}
\input{latex_files/ch4/4-4-extensibility.tex}

%% -- Use-case illustration ----------------------------------------------------
\section{Use-case illustration}
\input{latex_files/ch5/5-ma2.tex}

%% -- Summary/conclusions/discussion -------------------------------------------
\section{Summary and discussion} \label{sec:summary}
\input{latex_files/ch6/6-summary.tex}

%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

The results in this paper were obtained using \proglang{Python}~3.7.9
with the \pkg{ELFI}~0.8.3 package. The experiments have been executed
in a single machine with an Intel® Core™ i7-8750H Processor an with
Ubuntu 20.04 lts operating system.

\section*{Acknowledgments}

HP was funded by European Research Council grant 742158 (SCARABEE,
Scalable inference algorithms for Bayesian evolutionary epidemiology).

\clearpage
\bibliography{refs}

%% -----------------------------------------------------------------------------

\end{document}
