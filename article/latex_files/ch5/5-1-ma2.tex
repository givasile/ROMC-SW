\subsubsection*{Model Definition}

MA2 is a probabilistic model for time series analysis. The observation
at time \(t\) is given by,

\begin{gather} \label{eq:ma2}
y_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2}, \quad t=1, \ldots, T\\
\theta_1, \theta_2 \in \R, \quad  w_k \sim \mathcal{N}(0,1), k \in \mathbb{Z}
\end{gather}

\noindent
The r.v. \(w_{k} \sim \mathcal{N}(0,1) \) is white noise and the two
parameters of interest, \(\theta_1, \theta_2\), model the dependence
from the previous observations. The number of sequential observations
\(T\) is a constant and set to \(T=100\). For securing
that the inference problem is identifiable, i.e.\ the likelihood has
only one mode, we use the prior proposed by~\cite{Marin2012},

\begin{equation} \label{eq:ma2_prior}
p(\thetab) = p(\theta_1)p(\theta_2|\theta_1)
= \mathcal{U}(\theta_1;-2,2)\mathcal{U}(\theta_2;\theta_1-1, \theta_1+1)
\end{equation}

% \begin{figure}[ht]
%     \begin{center}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/mae2_prior_samples.png}
%     \end{center}
%     \caption[MA2 example, prior distribution.]{Prior distribution proposed by \cite{Marin2012}. The
%       samples follow a triangular shape.}
%   \label{fig:ma2_1}
% \end{figure}

\noindent
The observation vector \(\yb_0 = (y_1, \ldots, y_{100})\) is generated
with \(\thetab^*=(0.6, 0.2)\). The dimensionality of the output
\(\yb\) is high, therefore we use summary statistics. Considering that
the ouptput vector represents a time-series signal, we select the
autocovariances with \(\mathrm{lag}=1\) and \(\mathrm{lag}=2\), as shown in equations
\eqref{eq:ma2_summary_1} and \eqref{eq:ma2_summary_2}. The final
distance node is the squared Euclidean distance
\eqref{eq:ma2_summary_4}.

\begin{gather} \label{eq:ma2_summary_1}
  s_1(\yb) = \frac{1}{T-1} \sum_{t=2}^T y_ty_{t-1}\\ \label{eq:ma2_summary_2}
  s_2(\yb) = \frac{1}{T-2} \sum_{t=3}^T y_ty_{t-2} \\
  s(\yb) = (s_1(\yb), s_2(\yb))\\ \label{eq:ma2_summary_4}
  d = ||s(\yb) - s(\yb_0)||_2^2 
\end{gather}

\subsubsection*{Inference}

In order to show all the capabilities of the implementation, we
perform the inference (i) using the gradient-based optimizer, (ii)
using the Bayesian Optimisation scheme and (iii) fitting a Neural
Network (NN) as a surrogate model. We use the Rejection ABC algorithm
for evaluating all approaches. Replacing the typical quadratic
surrogate model with a NN serves as an illustrator of the
extensibility of our implementation. The replacement is done by coding
a custom optimisation function with a surrogate model of our own
preference, as shown in Chapter~\ref{subsec:extensibility}. For the
definition of the NN, we use the \pkg{MLPRegressor} class of the
\pkg{scikit-learn} package. Therefore, the NN substitutes the real
distance function \(d_i\) inside the proposal region
\(q_i~ \forall i\) at the inference phase i.e.~sampling,
computing an expectation and evaluating the posterior. In our example
we use a neural network of two hidden layers of 10 neurons each and we
train it sampling 500 examples from each proposal region.

In Figure \ref{fig:ma2_5}, we illustrate the acceptance region of the
same deterministic simulator, in the gradient-based and the Bayesian
optimisation case. The acceptance regions are quite similar even though the
different optimisation schemes lead to different optimal points.

In Figure \ref{fig:ma2_3}, we demonstrate the histograms of the
marginal posteriors, for each approach; (a) Rejection ABC (first
column), (b) ROMC with gradient-based optimisation (second column) (c)
ROMC with Bayesian optimisation (third column) and (d) ROMC with the
NN extension. We observe a significant agreement between the
different approaches. At Table \ref{tab:ma2} we present the empirical
mean \(\mu\) and standard deviation \(\sigma\) for each inference
approach and finally, in Figure \ref{fig:ma2_4}, we illustrate the
unnormalised posterior for the three different variations of the ROMC
method. The results show that all ROMC variations provide consistent
results between them and in comparison with the standard Rejection ABC
algorithm.

\begin{table}
\begin{center}
\begin{tabular}{ c|c|c|c|c }
\hline
& \(\mu_{\theta_1}\) & \(\sigma_{\theta_1}\) & \(\mu_{\theta_2}\) & \(\sigma_{\theta_2}\) \\
\hline \hline
Rejection ABC & 0.516 & 0.142 & 0.07 & 0.172 \\
\hline
ROMC (gradient-based) & 0.503 & 0.143 & 0.032 & 0.17 \\
\hline
ROMC (Bayesian optimisation) & 0.494 & 0.16 & 0.086 & 0.167 \\
\hline
ROMC (Neural Network) & 0.503 & 0.140 & 0.03 & 0.171 \\
\hline
\end{tabular}
\end{center}
\caption{Comparison of the samples obtained from the estimated
  posterior with (a) Rejection sampling and (b) the different versions
  of ROMC. We observe that the obtained samples share similar
  statistics along all methods. \label{tab:ma2}}
\end{table}

\begin{figure}[ht]
    \begin{center}
        % \input{./latex_files/images/chapter4/ma2_region_3.tex}
        % \input{./latex_files/images/chapter4/ma2_region_3_bo.tex}
        \includegraphics[width=0.49\textwidth]{./latex_files/images/chapter4/ma2_region_1.png}
        \includegraphics[width=0.49\textwidth]{./latex_files/images/chapter4/ma2_region_1_bo.png}
    \end{center}
  \caption[The acceptance region of a specific deterministic simulator.]{The acceptance region in a specific optimisation problem. In the left figure the region obtained with gradient-based optimiser and in the right one with Bayesian Optimisation.}
  \label{fig:ma2_5}
\end{figure}


\begin{figure}[ht]
  \begin{center}
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_rejection.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc_bo.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc_nn.tex}
    }\\
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_rejection.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc_bo.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc_nn.tex}
    }
    \end{center}
    \caption[MA2 example, evaluation of the marginal
    distributions.]{Histogram of the marginal posterior distributions
      using three different inference approaches; (a) in the first
      row, the samples are obtained using Rejection ABC sampling (b)
      in the second row, using ROMC with a gradient-based optimiser
      and (c) in the third row, using ROMC with Bayesian optimisation
      approach. The vertical (red) line represents the samples mean
      \(\mu\) and the horizontal (black) the standard deviation
      \(\sigma\).}
  \label{fig:ma2_3}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior.tex}
    }
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior_bo.tex}
    }
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior_nn.tex}
    }
    \end{center}
    \caption[MA2 example, posterior distribution.]{The unnormalised posterior distribution using the ROMC method with (a) a gradient-based optimisation (b) Bayesian Optimisation (c) gradient-based with a Neural Network as a surrogate model.}
  \label{fig:ma2_4}
\end{figure}


\subsubsection*{Parallelisation}

As already stated, it is straightforward for the two basic parts of
the training part to be parallelised. There is a batch of \(n_1\)
independent objectives functions to be optimised and \(n_1\) bounding
boxes to be built. Both tasks can be applied in a parallel utilising
all the available CPU cores. Our implementation supports
parallelisation using the built-in \proglang{Python} package
\pkg{multiprocess}. In Figure~\ref{fig:exec_parallel} we observe the
execution times for performing the inference. The parallel version
performs all tasks between 2.5 and 6 times faster compared to the
sequential; the optimisation problems are solved almost 6 times
faster, sampling 3.5 times faster and evaluating the posterior and
constructing the bounding boxes almost 2.5 times faster.

\begin{figure}[ht]
  \begin{center}
    \resizebox{.47\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/exec_time_solve.tex}
    }
    \resizebox{.49\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/exec_time_regions.tex}
    }
    \end{center}
    \caption[Execution time exploiting parallelisation]{In the first
      line, we compare the parallel and sequential execution of the
      training part and at the second line the inference part. At the
      left figure, we measure the execution time for sampling \(n_2=50\)
      points per region. At the right figure we measure the execution
      time for evaluating the posterior at a batch of \(50\) points.}
      \label{fig:exec_parallel}
\end{figure}

% \begin{figure}[ht]
%   \centering
%     \begin{minipage}[t]{.4\textwidth}
%       \begin{tikzpicture}
%         \begin{axis}[
%           title={Solve optimisation problems},
%           xlabel={\(n_1\)},
%           ylabel={seconds},
%           xmin=1, xmax=500,
%           ymin=0, ymax=7,
%           xtick={1,100,200,300,400,500},
%           ytick={0, 1, 2, 3, 4, 5, 6, 7},
%           legend pos=north west,
%           ymajorgrids=true,
%           grid style=dashed,
%           ]
%           \addplot[color=blue, mark=o] coordinates {
%             (1,0.05)(125,1.6)(250,3.5)(375,5)(500,6.8)
%           };
%           \addlegendentry{sequential}
%           \addplot[color=red, mark=o] coordinates {
%             (1,0.2)(125,0.6)(250,1)(375,1.5)(500,2)
%           };
%           \addlegendentry{parallel}          
%         \end{axis}
%       \end{tikzpicture}
%     \end{minipage}\hfill
%     \begin{minipage}[t]{.4\textwidth}
%       \begin{tikzpicture}
%         \begin{axis}[
%           title={Proposal regions},
%           xlabel={\(n_1\)},
%           ylabel={seconds},
%           xmin=1, xmax=500,
%           ymin=0, ymax=7,
%           xtick={1,100,200,300,400,500},
%           ytick={0, 1, 2, 3, 4, 5, 6, 7},
%           legend pos=north west,
%           ymajorgrids=true,
%           grid style=dashed,
%           ]
%           \addplot[color=blue, mark=o] coordinates {
%             (1,0.05)(125,1.6)(250,3.5)(375,5)(500,6.8)
%           };
%           \addlegendentry{sequential}
%           \addplot[color=red, mark=o] coordinates {
%             (1,0.2)(125,0.6)(250,1)(375,1.5)(500,2)
%           };
%           \addlegendentry{parallel}          
%         \end{axis}
%       \end{tikzpicture}
%     \end{minipage}\hfill
%     \begin{minipage}[t]{.4\textwidth}
%       \begin{tikzpicture}
%         \begin{axis}[
%           title={Sample},
%           xlabel={\(n_1\)},
%           ylabel={seconds},
%           xmin=1, xmax=500,
%           ymin=0, ymax=7,
%           xtick={1,100,200,300,400,500},
%           ytick={0, 1, 2, 3, 4, 5, 6, 7},
%           legend pos=north west,
%           ymajorgrids=true,
%           grid style=dashed,
%           ]
%           \addplot[color=blue, mark=o] coordinates {
%             (1,0.05)(125,1.6)(250,3.5)(375,5)(500,6.8)
%           };
%           \addlegendentry{sequential}
%           \addplot[color=red, mark=o] coordinates {
%             (1,0.2)(125,0.6)(250,1)(375,1.5)(500,2)
%           };
%           \addlegendentry{parallel}          
%         \end{axis}
%       \end{tikzpicture}
%     \end{minipage}\hfill
%     \begin{minipage}[t]{.4\textwidth}
%       \begin{tikzpicture}
%         \begin{axis}[
%           title={Evaluate posterior},
%           xlabel={\(n_1\)},
%           ylabel={seconds},
%           xmin=1, xmax=500,
%           ymin=0, ymax=7,
%           xtick={1,100,200,300,400,500},
%           ytick={0, 1, 2, 3, 4, 5, 6, 7},
%           legend pos=north west,
%           ymajorgrids=true,
%           grid style=dashed,
%           ]
%           \addplot[color=blue, mark=o] coordinates {
%             (1,0.05)(125,1.6)(250,3.5)(375,5)(500,6.8)
%           };
%           \addlegendentry{sequential}
%           \addplot[color=red, mark=o] coordinates {
%             (1,0.2)(125,0.6)(250,1)(375,1.5)(500,2)
%           };
%           \addlegendentry{parallel}          
%         \end{axis}
%       \end{tikzpicture}
%     \end{minipage}\hfill
%     \caption[Execution time exploiting parallelisation]{In the first
%       line, we compare the parallel and sequential execution of the
%       training part and at the second line the inference part. At the
%       left figure, we measure the execution time for sampling \(n_2=50\)
%       points per region. At the right figure we measure the execution
%       time for evaluating the posterior at a batch of \(50\) points.}
%       \label{fig:exec_parallel}
% \end{figure}