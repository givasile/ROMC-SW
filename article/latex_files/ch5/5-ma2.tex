In this section, we test the implementation using the second-order
moving average (MA2) example, which is one of the standard models of
\pkg{ELFI}. We perform the inference using three different versions of
ROMC; (i) using a gradient-based optimiser, (ii) using the Bayesian
Optimisation scheme and (iii) fitting a Neural Network as a surrogate
model. The later illustrates how to extend the implementation,
replacing part of ROMC with a user-defined component. Finally, we
measure the execution speed-up achieved by the parallelised version of
ROMC.

\subsubsection*{Model Definition}

MA2 is a probabilistic model for time series analysis. The observation
at time \(t\) is given by,

\begin{gather} \label{eq:ma2}
y_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2}, \quad t=1, \ldots, T\\
\theta_1, \theta_2 \in \R, \quad  w_k \sim \mathcal{N}(0,1), k \in \mathbb{Z}
\end{gather}

\noindent
The r.v. \(w_{k} \sim \mathcal{N}(0,1) \) is white noise and the two
parameters of interest, \(\theta_1, \theta_2\), model the dependence
from the previous observations. The number of sequential observations
\(T\) is a constant and set to \(T=100\). For securing
that the inference problem is identifiable, i.e.\ the likelihood has
only one mode, we use the prior proposed by~\cite{Marin2012},

\begin{equation} \label{eq:ma2_prior}
p(\thetab) = p(\theta_1)p(\theta_2|\theta_1)
= \mathcal{U}(\theta_1;-2,2)\mathcal{U}(\theta_2;\theta_1-1, \theta_1+1)
\end{equation}

% \begin{figure}[ht]
%     \begin{center}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/mae2_prior_samples.png}
%     \end{center}
%     \caption[MA2 example, prior distribution.]{Prior distribution proposed by \cite{Marin2012}. The
%       samples follow a triangular shape.}
%   \label{fig:ma2_1}
% \end{figure}

\noindent
The observation vector \(\yb_0 = (y_1, \ldots, y_{100})\) is generated
with \(\thetab^*=(0.6, 0.2)\). The dimensionality of the output
\(\yb\) is high, therefore we use summary statistics. Considering that
the ouptput vector represents a time-series signal, we select the
autocovariances with \(\mathrm{lag}=1\) and \(\mathrm{lag}=2\), as shown in equations
\eqref{eq:ma2_summary_1} and \eqref{eq:ma2_summary_2}. The final
distance node is the squared Euclidean distance
\eqref{eq:ma2_summary_4}.

\begin{gather} \label{eq:ma2_summary_1}
  s_1(\yb) = \frac{1}{T-1} \sum_{t=2}^T y_ty_{t-1}\\ \label{eq:ma2_summary_2}
  s_2(\yb) = \frac{1}{T-2} \sum_{t=3}^T y_ty_{t-2} \\
  s(\yb) = (s_1(\yb), s_2(\yb))\\ \label{eq:ma2_summary_4}
  d = ||s(\yb) - s(\yb_0)||_2^2 
\end{gather}

\subsubsection*{Inference}

In order to show all the capabilities of the implementation, we
perform the inference (i) using the gradient-based optimizer, (ii)
using the Bayesian Optimisation scheme and (iii) fitting a Neural
Network (NN) as a surrogate model. We use the Rejection ABC algorithm
for evaluating all approaches. Replacing the typical quadratic
surrogate model with a NN serves as an illustrator of the
extensibility of our implementation. The replacement is done by coding
a custom optimisation function with a surrogate model of our own
preference, as shown in Chapter~\ref{subsec:extensibility}. For the
definition of the NN, we use the \pkg{MLPRegressor} class of the
\pkg{scikit-learn} package. Therefore, the NN substitutes the real
distance function \(d_i\) inside the proposal region
\(q_i~ \forall i\) at the inference phase i.e.~sampling,
computing an expectation and evaluating the posterior. In our example
we use a neural network of two hidden layers of 10 neurons each and we
train it sampling 500 examples from each proposal region.

In Figure \ref{fig:ma2_5}, we illustrate the acceptance region of the
same deterministic simulator, in the gradient-based and the Bayesian
optimisation case. The acceptance regions are quite similar even though the
different optimisation schemes lead to different optimal points.

In Figure \ref{fig:ma2_3}, we demonstrate the histograms of the
marginal posteriors, for each approach; (a) Rejection ABC (first
column), (b) ROMC with gradient-based optimisation (second column) (c)
ROMC with Bayesian optimisation (third column) and (d) ROMC with the
NN extension. We observe a significant agreement between the
different approaches. At Table \ref{tab:ma2} we present the empirical
mean \(\mu\) and standard deviation \(\sigma\) for each inference
approach and finally, in Figure \ref{fig:ma2_4}, we illustrate the
unnormalised posterior for the three different variations of the ROMC
method. The results show that all ROMC variations provide consistent
results between them and in comparison with the standard Rejection ABC
algorithm.

\begin{table}
\begin{center}
\begin{tabular}{ c|c|c|c|c }
\hline
& \(\mu_{\theta_1}\) & \(\sigma_{\theta_1}\) & \(\mu_{\theta_2}\) & \(\sigma_{\theta_2}\) \\
\hline \hline
Rejection ABC & 0.516 & 0.142 & 0.07 & 0.172 \\
\hline
ROMC (gradient-based) & 0.503 & 0.143 & 0.032 & 0.17 \\
\hline
ROMC (Bayesian optimisation) & 0.494 & 0.16 & 0.086 & 0.167 \\
\hline
ROMC (Neural Network) & 0.503 & 0.140 & 0.03 & 0.171 \\
\hline
\end{tabular}
\end{center}
\caption{Comparison of the samples obtained from the estimated
  posterior with (a) Rejection sampling and (b) the different versions
  of ROMC. We observe that the obtained samples share similar
  statistics along all methods. \label{tab:ma2}}
\end{table}

\begin{figure}[ht]
    \begin{center}
        % \input{./latex_files/images/chapter4/ma2_region_3.tex}
        % \input{./latex_files/images/chapter4/ma2_region_3_bo.tex}
        \includegraphics[width=0.49\textwidth]{./latex_files/images/chapter4/ma2_region_1.png}
        \includegraphics[width=0.49\textwidth]{./latex_files/images/chapter4/ma2_region_1_bo.png}
    \end{center}
  \caption[The acceptance region of a specific deterministic simulator.]{The acceptance region in a specific optimisation problem. In the left figure the region obtained with gradient-based optimiser and in the right one with Bayesian Optimisation.}
  \label{fig:ma2_5}
\end{figure}


\begin{figure}[ht]
  \begin{center}
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_rejection.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc_bo.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc_nn.tex}
    }\\
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_rejection.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc_bo.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc_nn.tex}
    }
    \end{center}
    \caption[MA2 example, evaluation of the marginal
    distributions.]{Histogram of the marginal posterior distributions
      using three different inference approaches; (a) in the first
      row, the samples are obtained using Rejection ABC sampling (b)
      in the second row, using ROMC with a gradient-based optimiser
      and (c) in the third row, using ROMC with Bayesian optimisation
      approach. The vertical (red) line represents the samples mean
      \(\mu\) and the horizontal (black) the standard deviation
      \(\sigma\).}
  \label{fig:ma2_3}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior.tex}
    }
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior_bo.tex}
    }
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior_nn.tex}
    }
    \end{center}
    \caption[MA2 example, posterior distribution.]{The unnormalised posterior distribution using the ROMC method with (a) a gradient-based optimisation (b) Bayesian Optimisation (c) gradient-based with a Neural Network as a surrogate model.}
  \label{fig:ma2_4}
\end{figure}


\subsubsection*{Parallelisation}

As stated above, ROMC is a ridiculously parallelisable
method. Therefore, it is straightforward to parallelise the fitting
part, i.e. (i) solving the optimisation problems and (ii) estimating
the proposal regions. Our implementation supports exploiting all the
available CPU cores through the built-in \proglang{Python} package
\pkg{multiprocess}\footnote{https://docs.python.org/3/library/multiprocessing.html}. In
Figure~\ref{fig:exec_parallel} we observe the execution times for
performing the inference on the ma2 model; the parallel version
performs both tasks almost six times faster than the sequential. The
result is reasonable given that the experiments have run in a single
machine with the Intel® Core™ i7-8750H Processor, which has six
separate cores.


\begin{figure}[ht]
  \begin{center}
    \resizebox{.49\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/exec_time_solve.tex}
    }
    \resizebox{.49\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/exec_time_regions.tex}
    }
    \end{center}
    \caption[Execution time exploiting parallelisation]{Comparison
      between parallel and sequential execution of the fitting part of
      ROMC. We observe that the parallel version runs between 2.5 an 6
      times faster.}
      \label{fig:exec_parallel}
\end{figure}
