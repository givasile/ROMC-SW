The training part contains six methods.

\begin{Code}
---------------------------------- python ----------------------------------
>>> romc.solve_problems(n1,
                        use_bo=False,
                        optimizer_args=None,
                        seed=None)
----------------------------------------------------------------------------                    
\end{Code}

\noindent
This method (a) draws the nuisance variables, (b) defines the
optimisation problems and (c) solves them using either a
gradient-based optimiser or Bayesian optimisation. These tasks are
completed sequentially, as shown in
Figure~\ref{fig:romc_overview}. The definition of the optimisation
problems is performed by drawing \code{n1} integer numbers from a
discrete uniform distribution \(u_i \sim \mathcal{U}\{1,
2^{32}-1\}\). Each integer \(u_i\) is the seed used in \pkg{ELFI}'s
random simulator. The seeds \(u_i\) are computationally analogous to
the random variables \(\vb_i\), that were defined in the previous
chapter; setting the \code{seed} eliminates all randomness of the
simulator transforming it to a deterministic function. Setting the
argument \code{use_bo=True}, chooses the Bayesian Optimisation scheme
for obtaining \(\thetab_i^*\). In this case, apart from obtaining the
optimal points \(\thetab_i^*\), we also fit a GP as a surrogate model
\(\hat{d}_i\). In this scenario, in the following steps,
\(\hat{d}_i(\thetab)\) replaces \(d_i(\thetab)\) as the distance
function.

\begin{Code}
---------------------------------- python ----------------------------------
>>> romc.distance_hist(**kwargs)
----------------------------------------------------------------------------  
\end{Code}

\noindent
This function helps the user decide which threshold \(\epsilon\) to
use. It plots a histogram of the distances at the optimal point
\(d_i(\thetab_i^*) : \{i = 1, 2, \ldots, n_1\}\) or \(\hat{d}_i^*\) in case
\code{use_bo=True}. The function accepts all keyword arguments and
forwards them to the underlying \code{matplotlib.pyplot.hist()} function; in
this way the user may customise some properties of the histogram, such
as the number of bins or the range of values.

\begin{Code}
---------------------------------- python ----------------------------------  
>>> romc.estimate_regions(eps_filter,
                          use_surrogate=None,
                          region_args=None,
                          fit_models=False,
                          fit_models_args=None,
                          eps_region=None,
                          eps_cutoff=None)
----------------------------------------------------------------------------
\end{Code}

\noindent
This method constructs a bounding box around the optimal point
\(\thetab_i^* : i = 1, 2, \ldots, n_1\) following
Algorithm~\ref{alg:region_construction}. The Hessian matrix is
approximated based on the Jacobian
\(\hess_i = \jac_i^T \jac_i\)\footnote{The Jacobian matrix is computed
  on the output of the summary statistics
  \(\nabla_{\thetab} \Phi(M_d(\thetab, \vb_i))\) at \(\thetab_i^*\)} if
gradients have been used, or as the Hessian of the GP, in case of
Bayesian Optimisation. The eigenvectors are computed using the function
\code{numpy.linalg.eig()}, and used as the axes of the
bounding box. Afterwards, the limits along the
eigenvectors are computed with
Algorithm~\ref{alg:region_construction}.

\begin{Code}
---------------------------------- python ----------------------------------
>>> romc.fit_posterior(args*)  # training in a single call
>>> romc.visualize_region(i)   # acceptance region
>>> romc.compute_eps(quantile) # estimates eps
----------------------------------------------------------------------------
\end{Code}

\noindent
The function \code{fit_posterior} is a syntactic sugar for applying
\code{solve_problems} and \\ \code{estimate_regions} into a single
step. The function \code{visualize_region} can be used for plotting
the bounding box around the optimal point, when the parameter space is
one- or two-dimensional. The argument \code{i} is the index of the corresponding
optimisation problem i.e.\ \(i<n_1\). \code{compute_eps} returns the
appropriate distance value \(d_{i=\kappa}^*\) where
\(\kappa = \lfloor quantile \cdot n \rfloor\) from the collection
\(\{ d_i^* \} \forall i = \{1, \ldots, n\}\) where \(n\) is the number of
accepted solutions. It can be used to automate the selection of the
threshold \(\epsilon\), e.g.\ \code{eps=romc.compute_eps(quantile=0.9)}.


\subsubsection*{Example}


In the following snippet, we put together the routines described above
to perform the training part at our running example.

\begin{Code}
------------------------------ python snippet ------------------------------
  # Training (fitting) part
  n1 = 500 # number of optimisation problems
  seed = 21 # seed for solving the optimisation problems
  eps = .75 
  use_bo = False # set to True for switching to Bayesian optimisation

  # Training step-by-step
  romc.solve_problems(n1=n1, seed=seed, use_bo=use_bo)
  romc.theta_hist(bins=100) # plot hist to decide which eps to use

  eps = .75 # threshold for the bounding box based on histogram inspection
  romc.estimate_regions(eps=eps) # build the bounding boxes

  romc.visualize_region(i=1) # for inspecting visually the bounding box

  # Equivalent one-line command
  # romc.fit_posterior(n1=n1, eps=eps, use_bo=use_bo, seed=seed)
----------------------------------------------------------------------------  
\end{Code}

\begin{figure}[ht]
  \begin{center}    
    \input{latex_files/images/chapter3/example_gt.tex}
    \input{latex_files/images/chapter3/example_post.tex}
  \end{center}
  \caption[Histogram of distances at the one-dimensional example.]{Histogram of
    distances and visualisation of a specific region.}
  \label{fig:running_example_romc_inference}
\end{figure}
