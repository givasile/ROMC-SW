\begin{Code}
---------------------------------- python ----------------------------------  
>>> romc.compute_divergence(gt_posterior, bounds, step, distance)
>>> romc.compute_ess()
----------------------------------------------------------------------------
\end{Code}

\noindent
The utility \code{compute_divergence} estimate the divergence between
the ROMC approximation and the ground truth posterior. Since the
estimation is performed using the Riemann's approximation, the method
can only work in low dimensional spaces. As mentioned at the beginning
of this chapter, in a real-case scenario it is not expected the
ground-truth posterior to be available. In cases the ground truth
posterior is not availabe (as in real-scenarios), the user may select
the approximation obtained with any other inference approach for
comparing the two methods. The argument \code{step} defines the step
used in the Riemann's approximation and the argument \code{distance}
can be either \code{"Jensen-Shannon"} or \code{"KL-divergence"}.

The method \code{compute_ess} computes the Effective Sample Size
(ESS) using the following expression,

\begin{equation} \label{eq:ESS}
  ESS = \frac{(\sum_i w_i)^2}{\sum_i w_i^2}
\end{equation}

The ESS is a valuable measure of the \textbf{actual} sample size in
cases of weighted samples. For example, there are cases where in a big
population, a single sample with huge weight dominates. The ESS
provides a nice metric in these cases.

\begin{Code}
------------------------------ python snippet ------------------------------  
  # Evaluation part
  res = romc.compute_divergence(wrapper, distance="Jensen-Shannon")                                 
  print("Jensen-Shannon divergence: %.3f" % res)
  # Jensen-Shannon divergence: 0.035

  nof_samples = len(romc.result.weights)
  ess = romc.compute_ess()
  print("Nof Samples: %d, ESS: %.3f" % (nof_samples, ess))
  # Nof Samples: 19300, ESS: 16196.214
----------------------------------------------------------------------------  
\end{Code}
