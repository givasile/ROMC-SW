Second-order moving average (MA2) is a probabilistic model commonly
used for time series analysis. The observation at time $t$ is given
by:

\begin{gather} \label{eq:ma2}
y_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2}\\
\theta_1, \theta_2 \in \R, \quad  w_k \sim \mathcal{N}(0,1), k \in \mathbb{Z}
\end{gather}

\noindent
The random variable $w_{k} \sim \mathcal{N}(0,1), k \in \mathbb{Z}$
represent white noise and $\theta_1, \theta_2$, the two parameters of
interest, the dependence from the previous observations. The number of
consecutive observations $T$ is a hyper-parameter of the model that in
our case is set at $T=100$. For guaranteeing that the inference
problem is identifiable, i.e.\ the likelihood has only one mode, we
use the prior proposed by \cite{Marin2012} given in the equation
\eqref{eq:ma2_prior}:

\begin{equation} \label{eq:ma2_prior}
p(\thetab) = p(\theta_1)p(\theta_2|\theta_1)
= \mathcal{U}(\theta_1;-2,2)\mathcal{U}(\theta_2;\theta_1-1, \theta_1+1)
\end{equation}

% \begin{figure}[ht]
%     \begin{center}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/mae2_prior_samples.png}
%     \end{center}
%     \caption[MA2 example, prior distribution.]{Prior distribution proposed by \cite{Marin2012}. The
%       samples follow a triangular shape.}
%   \label{fig:ma2_1}
% \end{figure}

\noindent
The observation vector $\yb_0 = (y_1, \ldots, y_{100})$ is generated
with $\thetab^*=(0.6, 0.2)$. The dimensionality of the output $\yb$ is
high, therefore we use summary statistics. Considering that the
ouptput vector represents a time-series signal, we choose as summary
statistics the autocovariances with $lag=1$ and $lag=2$, as shown in
equations \eqref{eq:ma2_summary_1} and \eqref{eq:ma2_summary_2}. The
final distance node is the squared euclidean
distance \eqref{eq:ma2_summary_4}.

\begin{gather} \label{eq:ma2_summary_1}
  s_1(\yb) = \frac{1}{T-1} \sum_{t=2}^T y_ty_{t-1}\\ \label{eq:ma2_summary_2}
  s_2(\yb) = \frac{1}{T-2} \sum_{t=3}^T y_ty_{t-2} \\
  s(\yb) = (s_1(\yb), s_2(\yb))\\ \label{eq:ma2_summary_4}
  d = ||s(\yb) - s(\yb_0)||_2^2 
\end{gather}

\subsubsection*{Perform the inference}

As in the previous example, we perform the inference using both
optimisation alternatives (gradient-based and Bayesian optimisation)
and we compare the results at each step. In the end, we use the
Rejection ABC algorithm for evaluating both approximations. Rejection
ABC is a robust method in terms of accuracy and can be used for
baseline comparison.

In figure \ref{fig:ma2_5}, we illustrate the acceptance region of the
same deterministic simulator, i.e. with \code{seed=799981517}, in the
gradient-based and the Bayesian optimisation case. The acceptance
regions are similar in both cases. The optimal points are close and
the search directions are similar. This indicates that both
optimization scemes work robustly. Therefore, we observe that as we
move away from the optimal pint, the surrogate model fitted by the
Bayesian Optimisation tends to deviate from the real distance. This
divergence affects the bounding box approximation and explains some
small differences between the posterior approximations, as shown in
figure \ref{fig:ma2_3}. In figure \ref{fig:ma2_3}, we demonstrate the
histograms of the marginal posteriors, using the three different
methods; (a) Rejection ABC (first line), (b) ROMC with gradient-based
optimisation (second line) and (c) ROMC with Bayesian optimisation
(third line). Undoubtedly, we observe that there is a significant
similarity between the three approaches. The Rejection ABC inference
has been set to infer 10000 accepted samples, with threshold
$\epsilon=0.1$. At table \ref{tab:ma2} we present the empirical mean
$\mu$ and standard deviation $\sigma$ for each inference approach, for
verifying their agreement.

\begin{center} \label{tab:ma2}
\begin{tabular}{ c|c|c|c|c }
\hline
& $\mu_{\theta_1}$ & $\sigma_{\theta_1}$ & $\mu_{\theta_2}$ & $\sigma_{\theta_2}$ \\
\hline \hline
Rejection ABC & 0.516 & 0.142 & 0.07 & 0.172 \\
\hline
ROMC (gradient-based) & 0.504 & 0.143 & 0.033 & 0.171 \\
\hline
ROMC (Bayesian optimisation) & 0.502 & 0.166 & 0.079 & 0.168 \\
\hline
\end{tabular}
\end{center}

% \begin{figure}[H]
%     \begin{center}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/ma2_distance_hist.png}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/ma2_distance_hist_bo.png} \\
%     \end{center}
%     \caption[MA2 example, histogram of distances]{Histogram of distances
%       $d_i^*, i \in \{ 1, \ldots, n_1 \}$. The left graph corresponds
%       to the gradient-based approach and the right one to the Bayesian
%       optimisation approach.}
%   \label{fig:ma2_2}
% \end{figure}

\begin{figure}[ht]
    \begin{center}
        % \input{./latex_files/images/chapter4/ma2_region_3.tex}
        % \input{./latex_files/images/chapter4/ma2_region_3_bo.tex}
        \includegraphics[width=0.49\textwidth]{./latex_files/images/chapter4/ma2_region_1.png}
        \includegraphics[width=0.49\textwidth]{./latex_files/images/chapter4/ma2_region_1_bo.png}
    \end{center}
  \caption[The acceptance region of a specific deterministic simulator.]{The acceptance region in a specific optimisation problem. In the left figure the region obtained with gradient-based optimiser and in the right one with Bayesian Optimisation.}
  \label{fig:ma2_5}
\end{figure}


% \begin{figure}[h]
%     \begin{center}
%       \includegraphics[width=0.35\textwidth]{./latex_files/images/chapter4/mae2_hist_t1_rejection.png}
%       \includegraphics[width=0.35\textwidth]{./latex_files/images/chapter4/mae2_hist_t2_rejection.png}\\
%       \includegraphics[width=0.35\textwidth]{./latex_files/images/chapter4/mae2_hist_t1_romc.png}
%       \includegraphics[width=0.35\textwidth]{./latex_files/images/chapter4/mae2_hist_t2_romc.png}\\
%       \includegraphics[width=0.35\textwidth]{./latex_files/images/chapter4/mae2_hist_t1_romc_bo.png}
%       \includegraphics[width=0.35\textwidth]{./latex_files/images/chapter4/mae2_hist_t2_romc_bo.png}\\
%     \end{center}
%     \caption[MA2 example, evaluation of the marginal
%     distributions.]{Histogram of the marginal posterior distributions
%       using three different inference approaches; (a) in the first
%       row, the samples are obtained using Rejection ABC sampling (b)
%       in the second row, using ROMC with a gradient-based optimiser
%       and (c) in the third row, using ROMC with Bayesian optimization
%       approach. The vertical (red) line represents the samples mean
%       $\mu$ and the horizontal (black) the standard deviation
%       $\sigma$.}
%   \label{fig:ma2_3}
% \end{figure}


\begin{figure}[ht]
  \begin{center}
    \input{./latex_files/images/chapter4/mae2_hist_t1_rejection.tex}
    \input{./latex_files/images/chapter4/mae2_hist_t2_rejection.tex}\\
    \input{./latex_files/images/chapter4/mae2_hist_t1_romc.tex}
    \input{./latex_files/images/chapter4/mae2_hist_t2_romc.tex}\\
    \input{./latex_files/images/chapter4/mae2_hist_t1_romc_bo.tex}
    \input{./latex_files/images/chapter4/mae2_hist_t2_romc_bo.tex}\\
    \end{center}
    \caption[MA2 example, evaluation of the marginal
    distributions.]{Histogram of the marginal posterior distributions
      using three different inference approaches; (a) in the first
      row, the samples are obtained using Rejection ABC sampling (b)
      in the second row, using ROMC with a gradient-based optimiser
      and (c) in the third row, using ROMC with Bayesian optimization
      approach. The vertical (red) line represents the samples mean
      $\mu$ and the horizontal (black) the standard deviation
      $\sigma$.}
  \label{fig:ma2_3}
\end{figure}



% \begin{figure}[H]
%     \begin{center}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/mae2_romc_posterior.png}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/mae2_romc_posterior_bo.png}
%     \end{center}
%     \caption[MA2 example, approximate posterior.]{ROMC approximate posteriors using gradient-based approach
%       (left) and Bayesian optimisation approach (right).}
%   \label{fig:ma2_4}
% \end{figure}

