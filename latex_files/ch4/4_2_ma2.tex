Second-order moving average (MA2) is a probabilistic model commonly
used for time series analysis. The observation at time $t$ is given
by:

\begin{gather} \label{eq:ma2}
y_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2}\\
\theta_1, \theta_2 \in \R, \quad  w_k \sim \mathcal{N}(0,1), k \in \mathbb{Z}
\end{gather}

\noindent
The random variable $w_{k} \sim \mathcal{N}(0,1), k \in \mathbb{Z}$
represent white noise and $\theta_1, \theta_2$, the two parameters of
interest, the dependence from the previous observations. The number of
consecutive observations $T$ is a hyper-parameter of the model that in
our case is set at $T=100$. For guaranteeing that the inference
problem is identifiable, i.e.\ the likelihood has only one mode, we
use the prior proposed by \cite{Marin2012} given in the equation
\eqref{eq:ma2_prior}:

\begin{equation} \label{eq:ma2_prior}
p(\thetab) = p(\theta_1)p(\theta_2|\theta_1)
= \mathcal{U}(\theta_1;-2,2)\mathcal{U}(\theta_2;\theta_1-1, \theta_1+1)
\end{equation}

% \begin{figure}[ht]
%     \begin{center}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/mae2_prior_samples.png}
%     \end{center}
%     \caption[MA2 example, prior distribution.]{Prior distribution proposed by \cite{Marin2012}. The
%       samples follow a triangular shape.}
%   \label{fig:ma2_1}
% \end{figure}

\noindent
The observation vector $\yb_0 = (y_1, \ldots, y_{100})$ is generated
with $\thetab^*=(0.6, 0.2)$. The dimensionality of the output $\yb$ is
high, therefore we use summary statistics. Considering that the
ouptput vector represents a time-series signal, we choose as summary
statistics the autocovariances with $lag=1$ and $lag=2$, as shown in
equations \eqref{eq:ma2_summary_1} and \eqref{eq:ma2_summary_2}. The
final distance node is the squared euclidean
distance \eqref{eq:ma2_summary_4}.

\begin{gather} \label{eq:ma2_summary_1}
  s_1(\yb) = \frac{1}{T-1} \sum_{t=2}^T y_ty_{t-1}\\ \label{eq:ma2_summary_2}
  s_2(\yb) = \frac{1}{T-2} \sum_{t=3}^T y_ty_{t-2} \\
  s(\yb) = (s_1(\yb), s_2(\yb))\\ \label{eq:ma2_summary_4}
  d = ||s(\yb) - s(\yb_0)||_2^2 
\end{gather}

\subsubsection*{Perform the inference}

As in the previous example, we perform the inference using both
optimisation alternatives (gradient-based and Bayesian optimisation)
and, in the end, by extending the implementation with a Neural Network
surrogate model. In the end, we use the Rejection ABC algorithm for
evaluating our approaches; Rejection ABC is a robust method in terms
of accuracy and can be used for baseline comparison.


The implementation's extension is performed using a multi-layer
perceptron as local surrogate function, as illustrated in chapter
... This intervention can be done easily by coding a custom
optimisation function with the surrogate model of our own
preference. The surrogate model substitutes the real distance function
$d_i$ inside the proposal region $q_i \quad \forall i$ at the
inference phase i.e. sampling, computing an expectation and evaluating
the posterior. In our example we use a neural network of two hidden
layers of 10 neurons each and we train it in 500 examples for each
proposal region. 


In figure \ref{fig:ma2_5}, we illustrate the acceptance region of the
same deterministic simulator, in the gradient-based and the Bayesian
optimisation case. The acceptance regions are similar even though the
optimal points differ significantly due to the different optimisation
schemens.

In figure \ref{fig:ma2_3}, we demonstrate the histograms of the
marginal posteriors, using all four approaches; (a) Rejection ABC
(first column), (b) ROMC with gradient-based optimisation (second
column) (c) ROMC with Bayesian optimisation (third column) and (d)
ROMC with the Neural Network extendion. We observe that there is a
significant aggreement between the different approaches. The Rejection
ABC inference has been set to infer 10000 accepted samples, with
threshold $\epsilon=0.1$. At table \ref{tab:ma2} we present the
empirical mean $\mu$ and standard deviation $\sigma$ for each
inference approach. In figure \ref{fig:ma2_4}, we illustrate the
unnormalized posterior for the three different variations of the ROMC
method.

\subsubsection*{Extending the implementation}



\begin{center} \label{tab:ma2}
\begin{tabular}{ c|c|c|c|c }
\hline
& $\mu_{\theta_1}$ & $\sigma_{\theta_1}$ & $\mu_{\theta_2}$ & $\sigma_{\theta_2}$ \\
\hline \hline
Rejection ABC & 0.516 & 0.142 & 0.07 & 0.172 \\
\hline
ROMC (gradient-based) & 0.503 & 0.143 & 0.032 & 0.17 \\
\hline
ROMC (Bayesian optimisation) & 0.494 & 0.16 & 0.086 & 0.167 \\
\hline
ROMC (Neural Network) & 0.503 & 0.140 & 0.03 & 0.171 \\
\hline
\end{tabular}
\end{center}

% \begin{figure}[H]
%     \begin{center}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/ma2_distance_hist.png}
%       \includegraphics[width=0.48\textwidth]{./latex_files/images/chapter4/ma2_distance_hist_bo.png} \\
%     \end{center}
%     \caption[MA2 example, histogram of distances]{Histogram of distances
%       $d_i^*, i \in \{ 1, \ldots, n_1 \}$. The left graph corresponds
%       to the gradient-based approach and the right one to the Bayesian
%       optimisation approach.}
%   \label{fig:ma2_2}
% \end{figure}

\begin{figure}[ht]
    \begin{center}
        % \input{./latex_files/images/chapter4/ma2_region_3.tex}
        % \input{./latex_files/images/chapter4/ma2_region_3_bo.tex}
        \includegraphics[width=0.49\textwidth]{./latex_files/images/chapter4/ma2_region_1.png}
        \includegraphics[width=0.49\textwidth]{./latex_files/images/chapter4/ma2_region_1_bo.png}
    \end{center}
  \caption[The acceptance region of a specific deterministic simulator.]{The acceptance region in a specific optimisation problem. In the left figure the region obtained with gradient-based optimiser and in the right one with Bayesian Optimisation.}
  \label{fig:ma2_5}
\end{figure}


\begin{figure}[ht]
  \begin{center}
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_rejection.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc_bo.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t1_romc_nn.tex}
    }\\
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_rejection.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc_bo.tex}
    }
    \resizebox{.24\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_hist_t2_romc_nn.tex}
    }
    \end{center}
    \caption[MA2 example, evaluation of the marginal
    distributions.]{Histogram of the marginal posterior distributions
      using three different inference approaches; (a) in the first
      row, the samples are obtained using Rejection ABC sampling (b)
      in the second row, using ROMC with a gradient-based optimiser
      and (c) in the third row, using ROMC with Bayesian optimization
      approach. The vertical (red) line represents the samples mean
      $\mu$ and the horizontal (black) the standard deviation
      $\sigma$.}
  \label{fig:ma2_3}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior.tex}
    }
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior_bo.tex}
    }
    \resizebox{.32\columnwidth}{!}{%
      \input{./latex_files/images/chapter4/mae2_romc_posterior_nn.tex}
    }
    \end{center}
    \caption[MA2 example, posterior distribution.]{The unnormalized posterior distribution using the ROMC method with (a) a gradient-based optimization (b) Bayesian Optimization (c) gradient-based with a Neural Network as a surrogate model.}
  \label{fig:ma2_4}
\end{figure}
