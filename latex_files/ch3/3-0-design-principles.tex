For designing an extendable implementation of ROMC, we firstly define
ROMC as a sequence of algorithmic steps. The algorithmic view of ROMC
serves as the driver for the implementation at \pkg{ELFI}. At a high
level, ROMC can be split into the training and the inference part. In
general terms, the training part covers all steps for estimating the
proposal regions and the inference part calculates the weighted
samples. Algorithm~\ref{alg:romc_algorithm} defines ROMC formally;
Steps 2-10 (before the horizontal line) and steps 12-17 form the training 
part and the inference part, respectively. \textbf{HP comment Algorithm 1: I got a bit 
lost with $M_r(\thetab)$ (Line 1) and $M_d(\thetab, \vb=\vb_i)$ (Line 4). 
What is their relationship? Also, are Lines 13, 16 and 17 still under construction?
Maybe refer to Eq. (X)s in the Algorithm, if wanting to keep the Algorithm compact.}

\begin{algorithm}[!ht]
	\caption{ROMC}\label{alg:romc_algorithm}
	\begin{algorithmic}[1]
    \Procedure{ROMC}{$M_r(\thetab), n_1, n_2, \epsilon$}
    \For{$i \gets 1 \textrm{ to } n_1$}
      \State $\vb_i \sim p(\vb)$ \Comment{Draw nuisance variables}
      \State $d_i(\thetab) = M_d(\thetab, \vb=\vb_i)$ \Comment{Define distance function}
      \State $\thetab_i^* = \text{argmin}_{\thetab} d_i$, $d_i^*=d_i(\thetab_i^*)$ \Comment{Solve optimisation problem}
      \If{$d^*_i > \epsilon$}
        \State Go to 2 \Comment{Filter solution}
      \EndIf
      \State Define $q_i$ over $\mathcal{\hat{S}}$ \Comment{Estimate proposal area}
      \State (Optional) Fit $\Tilde{d}_i$ on $\mathcal{\hat{S}}$ \Comment{Fit surrogate model}
      \\\hrulefill
      \For{$j \gets 1 \textrm{ to } n_2$}      
        \State $\thetab_{ij} \sim q_i$, compute $w_{ij}$ \Comment{Sample}
      \EndFor
    \EndFor
    \State $E_{p(\thetab|\data)}[h(\thetab)]$ \Comment{Estimate an expectation}
    \State $p_{d,\epsilon}(\thetab)$ \Comment{Evaluate the unnormalised posterior}
    \EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection*{Training part}
\noindent
At the training (fitting) part, the goal is the estimation of the
proposal regions $\mathcal{\hat{S}}_i$. The tasks are; (a) sampling
the nuisance variables $\vb_i \sim p(\vb)$ for obtaining the
deterministic functions $d_i(\thetab)$, (b) defining the optimisation
problems $\min_{\thetab} \: d_i(\thetab)$, (c) obtaining
$\thetab_i^*$, $d_i^*$ (d) checking whether $d_i^* \leq \epsilon$ and
(e) computing the proposal distribution $q_i$.

If $d_i(\thetab)$ is differentiable, using a gradient-based method is
advised for obtaining $\thetab_i^*$ faster. In this case, the
gradients $\nabla_{\thetab} d_i$ gradients are approximated
automatically with finite-differences. This procedure requires about
two evaluations of $d_i$ for \emph{each} parameter $\theta_d$, which
can only work in low-dimensional problems. If $d_i(\thetab)$ is not
differentiable, the Bayesian Optimisation can be used as an
alternative solution. In this scenario, the training part becomes
slower due to fitting of the surrogate model and the blind
optimisation steps.

After obtaining the optimal points $\thetab^*$, the training part gets
completed by estimating the proposal
regions. Algorithm~\ref{alg:region_construction} describes the line
search approach for finding the region's boundaries. An important step
for the proposal region estimation is deciding the axes of the
bounding box i.e. the directions $\vb_d$ we follow for reaching the
boundaries. We approximate them as the direction of the highest
curvature of $d_i$ at $\thetab_i^*$. This estimation is given by the
eigenvalues of the Hessian matrix $\hessian_i$ of
$d_i$\footnote{Either the real distance $d_i$ or the Gaussian Process
  approximation $\hat{d}_i$} at $\thetab_i$. The Hessian matrix is
approximated numerically. In case where the distance function is the
Euclidean, the Hessian matrix can be also computed as
$\hessian_i = \jac_i^T\jac_i$, where $\jac_i$ is the Jacobian matrix
of the summary statistics $\Phi(M_d(\thetab, \vb=\vb_i))$ \textbf{is 
$M_d(\cdot, \cdot)$ the deterministic simulator function, or deterministic 
evaluation of the distance function? If so, what is it's relationship to 
the summary statistics?} at
$\thetab_i^*$. The approximation through the Jacobian matrix has the
computational advantage of using only first-order derivatives.

\subsubsection*{Inference Part}
Performing the inference includes one or more of the following three
tasks; (a) sampling from the posterior
$ \thetab_i \sim p_{d, \epsilon}(\thetab|\data)$, (b) computing an
expectation $E_{\thetab|\data}[h(\thetab)]$ or (c) evaluating the
unnormalised posterior $p_{d, \epsilon}(\thetab|\data)$. Sampling is
performed by getting $n_2$ samples from each proposal distribution
$q_i$. For each sample $\thetab_{ij}$, the distance
function\footnote{As before, if a surrogate model $\hat{d}$ is
  available, it can be utilised as the distance function.} is
evaluated for checking if it lies inside the acceptance
region. Algorithm~\ref{alg:sampling_GB} defines the steps for
computing a weighted sample. Computing the expectation is easy after weighted 
samples are obtained using the equation~\ref{eq:expectation}, so we do not 
discuss it separately. Evaluating the unnormalised posterior requires the
deterministic functions $g_i$ and the prior distribution
$p(\thetab)$\footnote{There is no need for solving the optimisation
  problems and building the proposal regions.}. The evaluation
requires iterating over all $g_i$ and evaluating the distance from the
observed data. \textbf{HP comment: Algorithm 2 - introduce the notation $||\thetab||$, or even 
better, define the variable separately, instead of using the same notation 
that was used for the norm - could it be $N$ if $\thetab \in \mathbb{R}^N$. 
Note that Line 20 would lead to an infinite loop.}

% \begin{minipage}{0.46\textwidth}
% \begin{algorithm}[H]
%     \centering
%     \caption{Training Part - Gradient-based. Requires $g_i(\thetab), p(\thetab)$}\label{alg:training_GB}
%     \begin{algorithmic}[1]
%       \For{$i \gets 1 \textrm{ to } n$}
%         \State Obtain $\thetab_i^*$ using a Gradient Optimiser
%         \If{$g_i(\thetab_i^*) > \epsilon$}
%         \State{go to} 1
%         \Else
%         \State Approximate $\jac_i^* = \nabla g_i(\theta)$ and $H_i \approx \jac^T_i\jac_i$
%         \State Use Algorithm~\ref{alg:region_construction} to obtain $q_i$
%         \EndIf      
%       \EndFor
%       \Return{$q_i, p(\theta), g_i(\theta)$}
%     \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \hfill
% \begin{minipage}{0.46\textwidth}
% \begin{algorithm}[H]
%     \centering
%     \caption{Training Part - Bayesian optimisation. Requires $g_i(\thetab), p(\thetab)$}\label{alg:training_GP}
%     \begin{algorithmic}[1]
%       \For{$i \gets 1 \textrm{ to } n$}
%         \State Obtain $\thetab_i^*, \hat{d}_i(\thetab)$ using a GP approach
%         \If{$g_i(\thetab_i^*) > \epsilon$}
%         \State{go to} 1
%         \Else
%         \State Approximate $H_i \approx \jac^T_i \jac_i$
%         \State Use Algorithm~\ref{alg:region_construction} to obtain $q_i$
%         \EndIf      
%       \EndFor
%       \Return{$q_i, p(\theta), \hat{d}_i(\theta)$}
%     \end{algorithmic}
% \end{algorithm}
% \end{minipage}

\begin{algorithm}[!ht]
	\caption{Approximation of $\mathcal{\hat{S}}_i$ with a bounding box; 
  Requires: a model of distance $d_i(\thetab)$, 
  an optimal point $\thetab_i^*$, 
  a number of refinements $K$, 
  a step size $\eta\_\text{start}$, 
  maximum iterations $M$ and 
  a curvature matrix $\hessian_i$ ($\jac_i^T\jac_i $ or GP Hessian)}\label{alg:region_construction}
	\begin{algorithmic}[1]
	\State Compute eigenvectors $\vb_{d}$ of $\hess_i$ {\scriptsize ($d = 1,\ldots,||\thetab ||)$}
	\For{$d \gets 1 \textrm{ to } ||\thetab||$}
		\State $\Tilde{\thetab} \gets \thetab_i^*$ \label{algstep:box_constr_start}
		\State $k \gets 0$
		\State $\eta \gets \eta\_\text{start}$ \Comment{Initialize $\eta$}
		\Repeat
          \State $j \gets 0$
        	\Repeat
            \State $\Tilde{\thetab} \gets \Tilde{\thetab} + \eta \ \vb_{d}$ \Comment{Large step size $\eta$.}
            \State $j \gets j + 1$
        	\Until{$d(f_i(\Tilde{\thetab}), \data) > \epsilon$ or $j \geq M$} \Comment{Check distance or maximum iterations}
        	\State $\Tilde{\thetab} \gets \Tilde{\thetab} - \eta \ \vb_{d}$
        	\State $\eta \gets \eta/2$ \Comment{More accurate region boundary}
        	\State $k \gets k + 1$
      \Until $k = K$
      \If{$\Tilde{\thetab} = \thetab_i^*$} \Comment{Check if no step has been done}
        \State $\Tilde{\thetab} \gets \Tilde{\thetab} + \dfrac{\eta\_{\text{start}}}{2^K} \vb_d$ \Comment{Then, make the minimum step}
      \EndIf
    	\State Set final $\Tilde{\thetab}$ as region end point. \label{algstep:box_constr_end}
    	\State Repeat steps~\ref{algstep:box_constr_start}~-~\ref{algstep:box_constr_end} for $\mathbf{v}_{d} = - \mathbf{v}_{d}$
	\EndFor
	\State Fit a rectangular box around the region end points and define $q_i$ as uniform distribution
	\end{algorithmic}
\end{algorithm}



\begin{algorithm}[H]
    \centering
    \caption{Sampling. Requires a function of distance $d_i$, the prior distribution $p(\thetab)$, the proposal distribution $q_i$}\label{alg:sampling_GB}
    \begin{algorithmic}[1]
      \State $\thetab_{ij} \sim q_i$
          \If {$d_i(\thetab_{ij}) > \epsilon$}
            \State Go to 2 \Comment{Reject sample}
          \Else {}
            \State $w_{ij} = \frac{p(\thetab_{ij})}{q(\thetab_{ij})}$ \Comment{Compute weight}
            \State Store $(w_{ij}, \thetab_{ij})$ \Comment{Store weighted sample}
          \EndIf
    \end{algorithmic}
\end{algorithm}
