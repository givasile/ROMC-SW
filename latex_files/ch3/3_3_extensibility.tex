ROMC can be conceived as a meta-algorithm; it describes a sequence of
steps for approximating the posterior distribution, without
\emph{explicitly} enforcing the methods that solve these steps. Even
though for each step a specific algotihm is proposed
by~\cite{Ikonomov2019}, in general ROMC works well if a practitioner
thinks of an alternative way of approaching a specific
step. Considering this particularity, we designed the implementation
to support such \emph{extensibility} capability.

We have specified four specific points where a user may intervene with
their custom modules; (a) the gradient-based solver, (b) the Bayesian
Optimisation solver, (c) the bounding box region construction and (d)
the fitting of the local surrogate models. Each of the tasks above may
be approached with completely different algorithms than the ones we
propose, without the rest of the method to change.

The four replacable parts described above, are solved using the four
methods of the \linebreak \code{OptimisatioProblem} class;
\code{solve_gradients(**kwargs)}, \code{solve_bo(**kwargs)},
\linebreak \code{build_region(**kwargs)},
\code{fit_local_surrogate(**kwargs)}. Therefore, the user should
create a custom class that inherits the basic
\code{OptimisatioProblem} class and overwrites one or more of the
above four functions with custom ones. We illustrate that in the
following example; suppose we want to blindly set the proposal region
as an n-dimensional bounding box around the optimal point. The
following snippet illustrates how:

\begin{Code}
---------------------------------- python ----------------------------------
from elfi.methods.inference.romc import OptimisationProblem
from elfi.methods.utils import NDimBoundingBox

bounds = [(-2.5, 2.5)]
dim = data.shape[-1]

class customOptim(OptimisationProblem):
    def __init__(self, **kwargs):
        super(customOptim, self).__init__(**kwargs)
        
    def build_region(self, **kwargs):
        # compute bounding box
        rotation = np.eye(1)
        theta_0 = self.result.x_min
        
        bounding_box = np.array([[theta_0[0]-1, theta_0[0]+1]])
        eps = kwargs["eps_region"]
        
        self.regions = [NDimBoundingBox(rotation, theta_0, bounding_box, eps)]
        
        # update state
        self.eps_region = kwargs["eps_region"]
        self.state["region"] = True
        return True

# Defines the ROMC inference method
romc = elfi.ROMC(dist, bounds, custom_optim_class=customOptim)
----------------------------------------------------------------------------    
\end{Code}

In the same way, the user may replace the other three functionalities,
respecting the side-effects of each one. In the following four
snippets we present the signature of each method. 


\begin{Code}
---------------------------------- python ----------------------------------
def solve_gradients(self, **kwargs):
  self.state["attempted"] = True

  # custom solution procedure
  
  if success:
    self.result = RomcOpimisationResult(res.x, res.y, jac, hess_inv)
    self.state["solved"] = True
    return True
  else:
    return False
----------------------------------------------------------------------------    
def solve_bo(self, **kwargs):
  self.state["attempted"] = True

  # custom procedure

  if success:
    self.result = RomcOpimisationResult(x, y)
    self.surrogate <- Callable
    self.state["solved"] = True
    self.state["has_fit_surrogate"] = True
    return True
  else:
    return False
----------------------------------------------------------------------------    
def build_region(self, **kwargs):
    self.eps_region = eps_region

    if success:
        # construct region
        self.regions <- List[NDimBoudningBox]
        self.state["region"] = True
        return True
    else:
        return False
----------------------------------------------------------------------------    
def fit_local_surrogate(self, **kwargs):
  if success:
    self.local_surrogate = local_surrogates
    self.state["local_surrogates"] = True
    return True
  else:
    return False
----------------------------------------------------------------------------    
\end{Code}


\begin{Code}
---------------------------------- python ----------------------------------
class RomcOpimisationResult:
    def __init__(self, x_min, f_min, jac=None, hess=None, hess_inv=None):
        Parameters
        ----------
        x_min: np.ndarray (D,) or float, the minimum point
        f_min: float, distance at the minimum point
        jac: np.ndarray (D,), gradients at the minimum point
        hess_inv: np.ndarray (DxD), inverse Hessian at the minimum point
        """
----------------------------------------------------------------------------    
class NDimBoundingBox:
    def __init__(self, rotation, center, limits, eps_region):
        Parameters
        ----------
        rotation: np.array (D,D),  rotation matrix for the bounding box
        center: np.array (D,) center of the bounding box
        limits: np.ndarray, shape: (D,2), the limits of the bounding box
----------------------------------------------------------------------------    
\end{Code}

% Let's say we have observed that the local area around $\theta_i^*$ is
% too complex to be represented by a simple quadratic model\footnote{as
%   in the current implementation}. Hence, the user selects a neural
% network as a good alternative. In the following snippet, we
% demonstrate how they could implement this enhancement without much
% effort; (a) they have to develop the neural network using the package
% of their choice (b) they must create a custom optimisation class which
% inherits the basic \code{OptimisationClass} and (c) they have to
% overwrite the \code{fit_local_surrogate} routine, with one that
% sets the neural network's prediction function as the
% \code{local_surrogate} attribute. The argument \code{**kwargs}
% may be used for passing all the important arguments, e.g.\ training
% epochs, gradient step etc. If, for example, they would like to set the
% size of the training set dynamically, we may replace \code{x =
%   self.regions[0].sample(30)} with \code{x =
%   self.regions[0].sample(kwargs["nof_examples"])}. Finally, they must
% pass the custom optimisation class, when calling the \code{ROMC}
% method.

% \begin{Code}
% ------------------------------ python snippet ------------------------------  
%   class NeuralNetwork:
%       def __init__(self, **kwargs):
%           # set the input arguments

%       def train(x, y):
%           # training code

%       def predict(x):
%           # prediction code

%   # Inherit the base optimisation class
%   class customOptim(elfi.methods.parameter_inference.OptimisationProblem):
%       def __init__(self, **kwargs):
%           super(customOptim, self).__init__(**kwargs)

%       # overwrite the function you want to replace
%       def fit_local_surrogate(**kwargs):
%           # init and train the NN
%           x = self.regions[0].sample(30) # 30 training points
%           y = [np.array([self.objective(ii) for ii in x])]
%           nn = NeuralNet()
%           nn.train(x,y)

%           # set the appropriate attribute
%           self.local_surrogate = nn.predict

%           # update the state
%           self.state["local_surrogate"] = True

%   # pass the custom inference method as argument
%   romc = elfi.ROMC(dist, bounds, custom_optim_class=customOptim)
% ----------------------------------------------------------------------------
% \end{Code}
