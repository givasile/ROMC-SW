An implicit or simulator-based model is a parameterised stochastic
data generating mechanism. The key characteristic of these models is
that we can sample, or simulate, data points, but that we cannot
evaluate the likelihood of a specific set of observations
$\data$. Formally, a simulator-based model is a parameterised family
of probability density functions $\{ p(\yb|\thetab) \}_{\thetab}$,
whose closed-form is either unknown or computationally
intractable. Whereas evaluating $p(\yb|\thetab)$ is intractable,
sampling is feasible. Practically, a simulator can be understood as a
random black-box machine (computer code) $M_r$ that generates samples
$\yb$ in a stochastic manner for any given a set of parameters
$\thetab$. We can isolate the randomness by introducing the stochastic
nuisance variables $\vb \sim p(\vb)$ so that the simulator becomes a
deterministic mapping $M_d$ that maps $(\thetab, \vb)$ to the data
$\yb$. The distribution $p(\vb)$ is determined by the random draws
performed when running $M_r$. Importantly, we can fix the value of
$\vb$ while changing $\thetab$ by fixing the random seed used with
$M_r$.

Simulator-based models provide considerable modelling freedom; any
physical process that can be conceptualised as a computer program of
finite steps can be modelled as a simulator-based model without any
compromise. The modelling freedom allows for any amount of hidden
(unobserved) internal variables or rule-based decisions. Hence,
implicit models are often used to model physical phenomena in the
natural sciences such as genetics, epidemiology, or neuroscience, to
name a few examples. Further background on simulator-based models and
example applications can be found in the articles
by \citet{Gutmann2016, Lintusaari2017, Sisson2018, Cranmer2020}.


The modelling freedom of simulator-based models, however, comes at the
price of difficulties in inferring their parameters. The main
difficulty is that the likelihood function $L(\thetab) =
p(\data|\thetab)$ is intractable. To better see the sources of the
intractability, and to address them, we go back to the basic
characterisation of the likelihood as the (rescaled) probability for a
parameter of the model to generate data $\yb$ that is similar to the
observed data $\data$. More formally, the likelihood $L(\thetab)$
equals
\begin{equation} \label{eq:likelihood}
  L(\thetab) = \lim_{\epsilon \to 0} c_\epsilon \int_{\yb \in B_{d,\epsilon}(\data)} p(\yb|\thetab)d\yb =
  \lim_{\epsilon \to 0} c_\epsilon \Pr(M_r(\thetab) \in \region(\data))
\end{equation}
where $c_\epsilon$ is a proportionality factor that depends on
$\epsilon$ and $\region(\data)$ is an $\epsilon$ region around $\data$
that is defined via a distance function $d$, i.e.\ $\region(\data)
:= \{\yb: d(\yb, \data) \leq \epsilon \}$. 


The basic characterisation of the likelihood in \eqref{eq:likelihood}
highlights two sources of intractability: The first is the computation
of the probability $\Pr(M_r(\thetab) \in \region(\data))$, the second
is the limit of $\epsilon \to 0$.  Whilst the probability can be
approximated with samples, this becomes computationally infeasible if
$\epsilon$ is too small. Hence, a large class of inference methods
work with $\epsilon >0$, which leads to the approximate likelihood
function $L_{d, \epsilon}(\thetab)$,
\begin{equation} \label{eq:approx_likelihood}
  L_{d, \epsilon}(\thetab) = \Pr(\yb \in \region(\data)), \quad \text{where  } \epsilon > 0.
\end{equation}
Most often in likelihood-free inference, we are interested in the posterior distribution of the parameters $\thetab$ given some observed data. The approximate likelihood function in \eqref{eq:approx_likelihood} implies the approximate posterior  $p_{d,\epsilon}(\thetab|\data)$, 
\begin{equation} \label{eq:approx_posterior}
  p_{d,\epsilon}(\thetab|\data) \propto L_{d, \epsilon}(\thetab) p(\thetab).
\end{equation}

The approximation in \eqref{eq:approx_likelihood} is by no means the
only strategy to deal with the intractabilities of the likelihood
function in \eqref{eq:likelihood}. Other strategies include modelling
the (stochastic) relationship between $\thetab$ and $\yb$, and its
reverse, or framing likelihood-free inference as a ratio estimation
problem \citep[see e.g.\ ][]{Blum2010, Wood2010, Papamakarios2016,
Thomas2016, Papamakarios2019, Chen2019, Thomas2020, Hermans2020}. However, both OMC and robust OMC, which we introduce next,
are based on the approximation in \eqref{eq:approx_likelihood}.

% likelihood of a specific parameter configuration $\thetab$ is equal to
% the probability that the simulator will produce outputs infinitively
% close to the observations $\data$, using this configuration. For
% continuous cases, the probability of perfectly replicating the
% observed data becomes zero. In this scenario a relaxation must be
% introduced by accepting simulated data that fall close to the data
% i.e. in a region $B_{d,\epsilon}(\data)$ around $\data$, where
% $\epsilon > 0$. The region can be defined as
% $\region (\data) := \{\yb: d(\yb, \data) \leq \epsilon \}$ where
% $d(\cdot, \cdot)$ can represent any valid distance.

% This relaxation introduces the approximate likelihood,


% and the approximate posterior,

