For designing an extendable implementation of ROMC, we firstly define
ROMC as a sequence of algorithmic steps. The algorithmic view of ROMC
serves as the driver for the implementation at \pkg{ELFI}. At a high
level, ROMC can be split into the training and the inference part. In
general terms, the training part covers all steps for estimating the
proposal regions and the inference part for getting weighted
samples. Algorithm~\ref{alg:romc_algorithm} defines ROMC formally;
Steps 2-10 (before the vertical line) form the training part, while
steps 12-17 the inference part.

\begin{algorithm}[!ht]
	\caption{ROMC}\label{alg:romc_algorithm}
	\begin{algorithmic}[1]
    \Procedure{ROMC}{$M_r(\thetab), n_1, n_2, \epsilon$}
    \For{$i \gets 1 \textrm{ to } n_1$}
      \State $\vb_i \sim p(\vb)$ \Comment{Draw nuisance variables}
      \State $d_i(\thetab) = M_d(\thetab, \vb=\vb_i)$ \Comment{Define distance function}
      \State $\thetab_i^* = \text{argmin}_{\thetab} d_i$, $d_i^*=d_i(\thetab_i^*)$ \Comment{Solve optimization problem}
      \If{$d^*_i > \epsilon$}
        \State Go to 2 \Comment{Filter solution}
      \EndIf
      \State Define $q_i$ over $\mathcal{\hat{S}}$ \Comment{Estimate proposal area}
      \State (Optional) Fit $\Tilde{d}_i$ on $\mathcal{\hat{S}}$ \Comment{Fit surrogate model}
      \\\hrulefill
      \For{$j \gets 1 \textrm{ to } n_2$}      
        \State $\thetab_{ij} \sim q_i$, compute $w_{ij}$ \Comment{Sample}
      \EndFor
    \EndFor
    \State $E_{p(\thetab|\data)}[h(\thetab)]$ \Comment{Estimate an expectation}
    \State $p_{d,\epsilon}(\thetab)$ \Comment{Evaluate the unnormalized posterior}
    \EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection*{Training part}
\noindent
At the training (fitting) part, the goal is the estimation of the
proposal regions $\mathcal{\hat{S}}_i$. The tasks are; (a) sampling
the nuisance variables $\vb_i \sim p(\vb)$ for obtaining the
deterministic functions $d_i(\thetab)$, (b) defining the optimisation
problems $\min_{\thetab} \: g_i(\thetab)$, (c) obtaining
$\thetab_i^*$, $d_i^*$ (d) checking whether $d_i^* \leq \epsilon$ and
(e) computing the proposal distribution $q_i$. If gradients are
available, using a gradient-based method is advised for obtaining
$\thetab_i^*$ much faster. In case $\nabla_{\thetab} g_i$ is available
in closed-form, the procedure is upgraded in terms of both accuracy
and efficiency; If closed-form description is not available, gradients
approximation with finite-differences requires two evaluations of
$g_i$ for \emph{each} parameter $\theta_d$, which can only work in
low-dimensional problems. If $g_i(\thetab)$ is not differentiable, the
Bayesian Optimisation can be used as an alternative solution. In this
scenario, the training part becomes slower due to fitting of the
surrogate model and the blind optimisation steps. After obtaning the
optimal points $\thetab^*$, the training part gets completed by
estimating the proposal regions;
Algorithm~\ref{alg:region_construction} describes the line search
approach for finding the region's boundaries.

\subsubsection*{Inference Part}
Performing the inference includes one or more of the following three
tasks; (a) sampling from the posterior
$ \thetab_i \sim p_{d, \epsilon}(\thetab|\data)$, (b) computing an
expectation $E_{\thetab|\data}[h(\thetab)]$ or (c) evaluating the
unnormalised posterior $p_{d, \epsilon}(\thetab|\data)$. Sampling is
performed by getting $n_2$ samples from each proposal distribution
$q_i$. For each sample $\thetab_{ij}$, the distance
function\footnote{As before, if a surrogate model $\hat{d}$ is
  available, it can be utilised as the distance function.} is
evaluated for checking if it lies inside the acceptance
region. Algorithm~\ref{alg:sampling_GB} defines the steps for
computing a weighted sample. Computing an expectation can be done
easily after weighted samples are obtained using the
equation~\ref{eq:expectation}, so we do not discuss it
separately. Evaluating the unnormalised posterior requires the
deterministic functions $g_i$ and the prior distribution
$p(\thetab)$\footnote{There is no need for solving the optimisation
  problems and building the proposal regions.}. The evaluation
requires iterating over all $g_i$ and evaluating the distance from the
observed data.

% \begin{minipage}{0.46\textwidth}
% \begin{algorithm}[H]
%     \centering
%     \caption{Training Part - Gradient-based. Requires $g_i(\thetab), p(\thetab)$}\label{alg:training_GB}
%     \begin{algorithmic}[1]
%       \For{$i \gets 1 \textrm{ to } n$}
%         \State Obtain $\thetab_i^*$ using a Gradient Optimiser
%         \If{$g_i(\thetab_i^*) > \epsilon$}
%         \State{go to} 1
%         \Else
%         \State Approximate $\jac_i^* = \nabla g_i(\theta)$ and $H_i \approx \jac^T_i\jac_i$
%         \State Use Algorithm~\ref{alg:region_construction} to obtain $q_i$
%         \EndIf      
%       \EndFor
%       \Return{$q_i, p(\theta), g_i(\theta)$}
%     \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \hfill
% \begin{minipage}{0.46\textwidth}
% \begin{algorithm}[H]
%     \centering
%     \caption{Training Part - Bayesian optimisation. Requires $g_i(\thetab), p(\thetab)$}\label{alg:training_GP}
%     \begin{algorithmic}[1]
%       \For{$i \gets 1 \textrm{ to } n$}
%         \State Obtain $\thetab_i^*, \hat{d}_i(\thetab)$ using a GP approach
%         \If{$g_i(\thetab_i^*) > \epsilon$}
%         \State{go to} 1
%         \Else
%         \State Approximate $H_i \approx \jac^T_i \jac_i$
%         \State Use Algorithm~\ref{alg:region_construction} to obtain $q_i$
%         \EndIf      
%       \EndFor
%       \Return{$q_i, p(\theta), \hat{d}_i(\theta)$}
%     \end{algorithmic}
% \end{algorithm}
% \end{minipage}

\begin{algorithm}[!ht]
	\caption{Approximation of $\mathcal{\hat{S}}_i$ with a bounding box; Requires, a model of distance $d_i(\thetab)$, optimal point $\thetab_i^*$, number of refinements $K$, step size $\eta\_\text{start}$, maximum iterations $M$ and curvature matrix $\hessian_i$ ($\jac_i^T\jac_i $ or GP Hessian)}\label{alg:region_construction}
	\begin{algorithmic}[1]
	\State Compute eigenvectors $\vb_{d}$ of $\hess_i$ {\scriptsize ($d = 1,\ldots,||\thetab ||)$}
	\For{$d \gets 1 \textrm{ to } ||\thetab||$}
		\State $\Tilde{\thetab} \gets \thetab_i^*$ \label{algstep:box_constr_start}
		\State $k \gets 0$
		\State $\eta \gets \eta\_\text{start}$ \Comment{Initialize $\eta$}
		\Repeat
          \State $j \gets 0$
        	\Repeat
            \State $\Tilde{\thetab} \gets \Tilde{\thetab} + \eta \ \vb_{d}$ \Comment{Large step size $\eta$.}
            \State $j \gets j + 1$
        	\Until{$d(f_i(\Tilde{\thetab}), \data) > \epsilon$ or $j \geq M$} \Comment{Check distance or maximum iterations}
        	\State $\Tilde{\thetab} \gets \Tilde{\thetab} - \eta \ \vb_{d}$
        	\State $\eta \gets \eta/2$ \Comment{More accurate region boundary}
        	\State $k \gets k + 1$
      \Until $k = K$
      \If{$\Tilde{\thetab} = \thetab_i^*$} \Comment{Check if no step has been done}
        \State $\Tilde{\thetab} \gets \Tilde{\thetab} + \dfrac{\eta\_{\text{start}}}{2^K} \vb_d$ \Comment{Then, make the minimum step}
      \EndIf
    	\State Set final $\Tilde{\thetab}$ as region end point. \label{algstep:box_constr_end}
    	\State Repeat steps~\ref{algstep:box_constr_start}~-~\ref{algstep:box_constr_end} for $\mathbf{v}_{d} = - \mathbf{v}_{d}$
	\EndFor
	\State Fit a rectangular box around the region end points and define $q_i$ as uniform distribution
	\end{algorithmic}
\end{algorithm}



\begin{algorithm}[H]
    \centering
    \caption{Sampling. Requires a function of distance $d_i$, the prior distribution $p(\thetab)$, the proposal distribution $q_i$}\label{alg:sampling_GB}
    \begin{algorithmic}[1]
      \State $\thetab_{ij} \sim q_i$
          \If {$d_i(\thetab_{ij}) > \epsilon$}
            \State Go to 2 \Comment{Reject sample}
          \Else {}
            \State $w_{ij} = \frac{p(\thetab_{ij})}{q(\thetab_{ij})}$ \Comment{Compute weight}
            \State Store $(w_{ij}, \thetab_{ij})$ \Comment{Store weighted sample}
          \EndIf
    \end{algorithmic}
\end{algorithm}
