Our description of OMC \citep{Meeds2015}
follows \citet{Ikonomov2019} who base their explanation of OMC on the
approximate likelihood function in \eqref{eq:approx_likelihood}. With
the indicator function
%
\begin{equation} \label{eq:indicator} \indicator{\region(\yb)}(\xb)=
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \xb \in \region(\yb) \\
      0 & \mbox{otherwise} 
    \end{array} \right. \end{equation}
%
we can write the approximate likelihood function $L_{d, \epsilon}(\thetab)$ in \eqref{eq:approx_likelihood} as
\begin{gather} \label{eq:approx_likelihood_omc}
  L_{d, \epsilon}(\thetab) = \Pr(\yb \in \region(\data)) =
  \int_{\yb \in B_{d,\epsilon}(\data)}p(\yb|\thetab)d\yb =
  \int_{\yb \in \R^D} \indicator{\region(\data)}(\yb)p(\yb|\thetab)d\yb
\end{gather}
which we can approximate as a sample average
\begin{equation}
L_{d, \epsilon}(\thetab) \approx \frac{1}{N} \sum_{i=1}^N \indicator{\region (\data)} (\yb_i)
  \text{ where } \yb_i = M_d(\thetab, \vb_i), \quad \vb_i \sim p(\vb). \label{eq:alt_view}
\end{equation}
We thus approximate the likelihood of a specific $\thetab$ by counting
the fraction of samples that lie inside the area around the
observations. The fact that we isolated the randomness of the
simulator via the $\vb_i$ has two crucial consequences: First, we can
sample the nuisance variables $\vb_i \sim p(\vb)$ only once and use
the same set of samples to approximate $L_{d,\epsilon}(\thetab)$ for
different values of $\thetab$. Secondly, since
$\yb_i=M_d(\thetab, \vb_i)$ is a function of $\thetab$, checking
whether $\yb_i$ is contained in region $\region (\data)$ for a
particular value of $\thetab$ is the same as checking whether
$\thetab$ is in the acceptance region $\mathcal{S}_i = \{ \thetab:
M_d(\thetab, \vb_i) \in \region(\data) \}$. This leads to the
likelihood approximation
\begin{equation}
L_{d, \epsilon}(\thetab) \approx \frac{1}{N} \sum_{i=1}^N \indicator{\mathcal{S}_i}(\thetab) \label{eq:alt_view_theta}
\end{equation}
and the corresponding approximate posterior
\begin{equation} \label{eq:approx_posterior_omc}
  p_{d,\epsilon}(\thetab|\data) \propto
  p(\thetab) \sum_i^N  \indicator{\mathcal{S}_i}(\thetab).
\end{equation}
As argued by \citet{Ikonomov2019}, this derivations provide a unique
perspective for likelihood-free inference by shifting the focus onto
the geometry of the acceptance regions $\mathcal{S}_i = \{ \thetab:
M_d(\thetab, \vb_i) \in \region(\data)\}$. Indeed, the task of
approximating the likelihood and/or posterior becomes a task of
characterising the sets $\mathcal{S}_i$.

OMC by \citet{Meeds2015} assumes that the distance $d$ is the
Euclidean distance between summary statistics $\Phi$ of the observed
and generated data, and that the $\mathcal{S}_i$ can be well
approximated by infinitesimally small ellipses. These assumptions
leads to an approximation of the posterior in terms of weighted
samples $\thetab_i^*$. In more detail, OMC considers the limit of
$\epsilon \to 0$ and represents the sets $\mathcal{S}_i$ by points
$\thetab_i^*$ which are those parameter configurations that achieve
the smallest distance between observed and simulated data for each
realisation $\vb_i \sim p(\vb)$, i.e.\ they are solutions to the
deterministic optimisation problems
\begin{equation} \label{eq:omc_opt_prob}
\thetab_i^* = \argmin_{\thetab} ||\Phi(\data)-\Phi(M_d(\thetab, \vb_i))||_2  , \quad \vb_i \sim p(\vb).
\end{equation}
The weighting for each $\thetab_i^*$ is proportional to the prior
density at $\thetab_i^*$ and inversely proportional to the determinant
of the Jacobian matrix of the summary statistics at $\thetab_i^*$. For
further details on OMC we refer the reader to \citep{Meeds2015,
Ikonomov2019}.

