Our description of OMC follows \citet{Ikonomov2019} who base their explanation of OMC on the approximate likelihood function in \eqref{eq:approx_likelihood}. With the indicator function
%
\begin{equation} \label{eq:indicator} \indicator{\region(\yb)}(\xb)=
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \xb \in \region(\yb) \\
      0 & \mbox{otherwise} 
    \end{array} \right. \end{equation}
%
we can write the approximate likelihood function $L_{d, \epsilon}(\thetab)$ in \eqref{eq:approx_likelihood} as
\begin{gather} \label{eq:approx_likelihood_omc}
  L_{d, \epsilon}(\thetab) = \Pr(\yb \in \region(\data)) =
  \int_{\yb \in B_\epsilon(\data)}p(\yb|\thetab)d\yb =
  \int_{\yb \in \R^D} \indicator{\region(\data)}(\yb)p(\yb|\thetab)d\yb
\end{gather}
which we can approximate as a sample average
\begin{equation}
L_{d, \epsilon}(\thetab) \approx \frac{1}{N} \sum_{i=1}^N \indicator{\region (\data)} (\yb_i)
  \text{ where } \yb_i = M_d(\thetab, \vb_i), \quad \vb_i \sim p(\vb). \label{eq:alt_view}
\end{equation}
We thus approximate the likelihood of a
specific $\thetab$ by counting the fraction of samples that lie inside the area around the
observations. The crucial point in \eqref{eq:alt_view} is that we may only sample the nuisance variables $\vb_i \sim p(\vb)$ once and use the sample set of samples to approximate $L_{d,\epsilon}(\thetab)$ for different values of $\thetab$. Based on this approach, we introduce the approximate posterior
\begin{equation} \label{eq:approx_posterior_omc}
  p_{d,\epsilon}(\thetab|\data)
  \propto p(\thetab) \sum_i^N \indicator{ \region(\data)} (\yb_i) \text{ where } \yb_i = M_d(\thetab, \vb_i), \quad \vb_i \sim p(\vb).
\end{equation}

\subsubsection*{Further approximations for sampling and computing expectations}

The posterior approximation in \eqref{eq:approx_posterior_omc} does
not provide any convenient way for drawing samples.\footnote{Weighted
  sampling could be performed in a straightforward fashion with
  importance sampling, using the prior as the proposal distribution
  $\thetab_i \sim p(\thetab)$. However, this approach has the same
  drawbacks as ABC rejection sampling; when the prior is wide or the
  dimensionality $D$ is high, drawing a sample with non-zero weight is
  rare, leading to either poor Effective Sample Size (ESS) or huge
  execution time.} In fact, the set
$\mathcal{S}_i = \{ \thetab: M_d(\thetab, \vb_i) \in \region(\data)
\}$ can represent any arbitrary shape in the D-dimensional Euclidean
space; it can be non-convex, can contain disjoint sets of $\thetab$
etc. We need some further simplification of the posterior for being
able to draw samples from it.

OMC proposes a quite drastic simplification of the posterior; it
squeezes all regions $\mathcal{S}_i$ into a single point
$\thetab_i^* \in \mathcal{S}_i$ attaching a weight $w_i$ proportional
to the volume of $\mathcal{S}_i$. For obtaining a
$\thetab_i^* \in \mathcal{S}_i$, a gradient based optimiser is used
for minimising $g_i(\thetab) = d(\data, f_i(\thetab))$ and the
estimation of the volume of $\mathcal{S}_i$ is done using the Hessian
approximation $\hess_i \approx \jac_i^{*T}\jac_i^*$, where $\jac_i^*$
is the Jacobian matrix of $g_i(\thetab)$ at $\thetab_i^*$. Hence,

\begin{gather} \label{eq:OMC_posterior}
    p(\thetab|\data) \propto p(\thetab) \sum_i^N w_i \delta(\thetab - \thetab_i^*)\\
  \thetab_i^* = \text{argmin}_{\thetab} \:g_i(\thetab) \\
  w_i \propto \frac{1}{\sqrt{det( \jac_i^{*T}\jac_i^*)}}
\end{gather}

The distribution \eqref{eq:OMC_posterior} provides weighted samples
automatically and an expectation can be computed easily with the
following equation,

\begin{equation}
  \label{eq:OMC_expectation}
  E_{p(\thetab|\data)}[h(\thetab)] = \frac{\sum_i^N w_i p(\thetab_i^*)h(\thetab_i^*)}{\sum_i^N w_i p(\thetab_i^*)}
\end{equation}
