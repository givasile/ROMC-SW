Before we define the likelihood approximation as introduced in the
OMC, approach lets define the indicator function based on
$\region(\yb)$. The indicator function $\indicator{\region(\yb)}(\xb)$
returns 1 if $\xb \in \region(\yb)$ and 0 otherwise. If
$d(\cdot,\cdot)$ is a formal distance, due to symmetry
$\indicator{\region(\yb)}(\xb) = \indicator{\region(\xb)}(\yb)$, so
the expressions can be used interchangeably.

\begin{gather} \label{eq:indicator} \indicator{\region(\yb)}(\xb)=
  \left\{
    \begin{array}{ll}
      1 & \mbox{if } \xb \in \region(\yb) \\
      0 & \mbox{otherwise} 
    \end{array} \right. \end{gather}

\noindent
Based on equation~\eqref{eq:approx_posterior} and the indicator
function as defined above~\eqref{eq:indicator}, we can approximate the
likelihood as:

\begin{gather} \label{eq:approx_likelihood_omc}
  L_{d, \epsilon}(\thetab) =
  \int_{\yb \in B_\epsilon(\data)}p(\yb|\thetab)d\yb =
  \int_{\yb \in \R^D} \indicator{\region(\data)}(\yb)p(\yb|\thetab)d\yb\\
  \approx \frac{1}{N} \sum_i^N \indicator{\region(\data)}(\yb_i),\text{ where }
  \yb_i \sim M_r(\thetab) \label{eq:init_view}\\
  \approx \frac{1}{N} \sum_i^N \indicator{\region (\data)} (\yb_i)
  \text{ where } \yb_i = M_d(\thetab, \vb_i), \vb_i \sim p(\vb) \label{eq:alt_view}
\end{gather}
%
This approach is quite intuitive; approximating the likelihood of a
specific $\thetab$ requires sampling from the data generator and count
the fraction of samples that lie inside the area around the
observations. Nevertheless, by using the approximation of equation
\eqref{eq:init_view} we need to draw $N$ new samples for each distinct
evaluation of $L_{d,\epsilon}(\thetab)$; this makes this approach
quite inconvenient from a computational point-of-view. For this
reason, we choose to approximate the integral as in equation
\eqref{eq:alt_view}; the nuisance variables are sampled once
$\vb_i \sim p(\vb)$ and we count the fraction of samples that lie
inside the area using the deterministic simulators
$M_d(\thetab, \vb_i) \: \forall i$. Hence, the evaluation
$L_{d,\epsilon}(\thetab)$ for each different $\thetab$ does not imply
drawing new samples all over again. Based on this approach, the
unnormalised approximate posterior can be defined as:

\begin{equation} \label{eq:approx_posterior_omc}
  p_{d,\epsilon}(\thetab|\data)
  \propto p(\thetab) \sum_i^N \indicator{ \region(\data)} (\yb_i)
\end{equation}

\subsubsection*{Further approximations for sampling and computing expectations}

The posterior approximation in \eqref{eq:approx_posterior_omc} does not
provide any obvious way for drawing samples. In fact, the set
$\mathcal{S}_i = \{ \thetab: M_d(\thetab, \vb_i) \in \region(\data) \}$ can
represent any arbitrary shape in the D-dimensional Euclidean space; it
can be non-convex, can contain disjoint sets of $\thetab$ etc. We need
some further simplification of the posterior for being able to draw
samples from it.

As a side-note, weighted sampling could be performed in a
straightforward fashion with importance sampling. Using the prior as
the proposal distribution $\thetab_i \sim p(\thetab)$ and we can
compute the weight as
$w_i = \frac{L_{d,\epsilon}(\thetab_i)}{p(\thetab_i)}$, where
$L_{d,\epsilon}(\thetab_i)$ is computed with the expression
\eqref{eq:approx_likelihood_omc}. This approach has the same drawbacks as
ABC rejection sampling; when the prior is wide or the dimensionality
$D$ is high, drawing a sample with non-zero weight is rare, leading to
either poor Effective Sample Size (ESS) or huge execution time.

The OMC proposes a quite drastic simplification of the posterior; it
squeezes all regions $\mathcal{S}_i$ into a single point
$\thetab_i^* \in \mathcal{S}_i$ attaching a weight $w_i$ proportional
to the volume of $\mathcal{S}_i$. For obtaining a
$\thetab_i^* \in \mathcal{S}_i$, a gradient based optimiser is used
for minimising $g_i(\thetab) = d(\data, f_i(\thetab))$ and the
estimation of the volume of $\mathcal{S}_i$ is done using the Hessian
approximation $\hess_i \approx \jac_i^{*T}\jac_i^*$, where $\jac_i^*$
is the Jacobian matrix of $g_i(\thetab)$ at $\thetab_i^*$. Hence,

\begin{gather} \label{eq:OMC_posterior}
    p(\thetab|\data) \propto p(\thetab) \sum_i^N w_i \delta(\thetab - \thetab_i^*)\\
  \thetab_i^* = \text{argmin}_{\thetab} \:g_i(\thetab) \\
  w_i \propto \frac{1}{\sqrt{det( \jac_i^{*T}\jac_i^*)}}
\end{gather}

The distribution \eqref{eq:OMC_posterior} provides weighted samples
automatically and an expectation can be computed easily with the
following equation,

\begin{equation}
  \label{eq:OMC_expectation}
  E_{p(\thetab|\data)}[h(\thetab)] = \frac{\sum_i^N w_i p(\thetab_i^*)h(\thetab_i^*)}{\sum_i^N w_i p(\thetab_i^*)}
\end{equation}
