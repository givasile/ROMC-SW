The simplifications introduced by OMC, although beneficial from a
computational point-of-view, they suffer from some drawbacks:

\begin{itemize}
\item The whole acceptable region $\mathcal{S}_i$
  shrinks to a single point $\thetab_i^*$/. This simplification adds
  significant apprimation error when then the area $\mathcal{S}_i$ is relatively big.
\item The weight $w_i$ is computed based only at the curvature at the
  point $\thetab_i^*$. Using only local information for estimating the weight can also be misleading. For example, if $g_i$ is almost flat at $\thetab_i^*$, i.e. $\text{det}(\jac_i^{*T}\jac_i^*) \rightarrow 0, then \Rightarrow w_i
  \rightarrow \infty$ dominating the posterior.
\item There is no way to solve the optimisation problem
  $\thetab_i^* = \text{argmin}_{\thetab} \: g_i(\thetab)$ when $g_i$
  is not differentiable.
\end{itemize}

\subsubsection{Sampling and computing expectation in ROMC}

ROMC resolves the failure-modes of OMC mainly by approaching the
acceptance regions $\mathcal{S}_i$ in a more sophisticated
way. Instead of collapsing the acceptance regions into single points,
it approximates them with a bounding box region. A uniform distribution
defined on the bounding box area, is then used as the \emph{proposal
  distribution} for importance sampling. If $q_i$ is the uniform
distribution defined on the $i-th$ bounding box, weighted sampling is
performed by drawing samples from $q_i$ and assigning the weights as
in~\eqref{eq:sampling}.

\begin{gather}
  \thetab_{ij} \sim q_i \\  \label{eq:sampling}
  w_{ij} = \frac{\indicator{\region(\data)}(M_d(\thetab_{ij}, \vb_i)) p(\thetab_{ij})}{q(\thetab_{ij})}
\end{gather}

\noindent

Having defined the procedure for obtaining weighted samples, any
expectation $E_{p(\thetab|\data)}[h(\thetab)]$, can be approximated
as,

\begin{equation} \label{eq:expectation}
  E_{p(\thetab|\data)}[h(\thetab)] \approx \frac{\sum_{ij} w_{ij} h(\thetab_{ij})}{\sum_{ij} w_{ij}}
\end{equation}


The constrution of the proposal regions $q_i$ is performed in two
steps; (a) solving the deterministic optimisation problems as in OMC
and (b) constructing the bounding box regions.

\subsubsection*{Solving the deterministic optimisation problems}

For each set of nuisance variables $\vb_i, i = \{1,2,\ldots,n_1 \}$ a
deterministic function is defined as
$f_i(\thetab) = M_d(\thetab,\vb_i)$. We look for a point
$\thetab_* : d(f_i(\thetab_*), \data) < \epsilon$. This point can be
obtained by solving the the following optimisation problem:

\begin{subequations}
\begin{alignat}{2}
&\!\min_{\thetab}        &\qquad& g_i(\thetab) = d(\data,  f_i(\thetab))\label{eq:optProb}\\
&\text{subject to} &      & g_i(\thetab) \leq \epsilon
\end{alignat}
\end{subequations}
%
The optimisation problem can be treated as unconstrained, accepting
the optimal point $\thetab_i^* = \text{argmin}_{\thetab} g_i(\thetab)$
only if $g_i(\thetab_i^*) < \epsilon$. The nature of the generative
model $M_r(\thetab)$, specifies the properties of the objective
function $g_i$. If $g_i$ is differentiable any gradient-based
iterative algorithm can be used for~\ref{eq:optProb}. The gradients
$\nabla_{\thetab} g_i$ can be either provided in closed form or
approximated by finite differences. Alternatively, if $g_i$ is not
differentiable, the Bayesian Optimisation approach provides an
alternative choice~\cite{Shahriari2016}. In this case, apart from
obtaining an optimal $\thetab_i^* $, a surrogate model $\hat{d}_i$ of
the distance $g_i$ is also fitted; the approximate model can then
substitute the actual model in the following steps, e.g. construction
of bounding box, sampling, for accelerating the method.

\subsubsection*{Proposal region computation}

After obtaining a $\thetab_i^*$ such that
$g_i(\thetab_i^*) < \epsilon$, we need to construct a bounding box
area $\mathcal{\hat{S}}_i$ around it. The bounding box must contain
the acceptance region around $\thetab_i^*$, formally
$\{ \thetab : g_i(\thetab) < \epsilon$,
$d(\thetab, \thetab_i^*) < M \}$. The second condition
$d(\thetab, \thetab_i^*) < M$ describes that if
$\mathcal{S}_i := \{ \thetab : g_i(\thetab) < \epsilon \} $ contains a
number of disjoint sets of $\thetab$ that they fulfill
$g_i(\thetab) < \epsilon$, we want the bounding box to match only the
set that contains $\thetab_i^*$. We seek for a bounding box that is as
tight as possible to the local acceptance region for ensuring high
acceptance rate in the importance sampling.

Bounding boxes are built in two steps. Firstly, the search directions
$\mathbf{v}_d$ are computed as the eigenvectors of the curvature at
$\thetab_i^*$ and then the limits are obtained with a line-search
method looking for the smallest
$\kappa: g_i(\thetab_i^* + \kappa \vb_d) \geq
\epsilon$\footnote{$-\kappa$ is used as well for the opposite
  direction along the search line}. The
Algorithm~\ref{alg:region_construction} describes the method
in-depth. After limits are found along all directions the uniform
distribution $q_i$ can be defined and is used as the proposal region
for importance sampling.


\subsubsection*{Fitting a local surrogate model $\hat{g}_i$}

After the construction of the bounding box $\mathcal{\hat{S}}_i$, we
are no longer interested in $g_i(\thetab)$ outside
$\mathcal{\hat{S}}_i$. In the inference steps (e.g. sampling,
evaluating the posterior), we evaluate $g_i$ for checking if a sample
from the bounding box belongs to the acceptance region. Instead of
running the deterministic simulator $M_d(\thetab_{ij}, \vb_i)$ for
evaluating $g_i$, ROMC proposes fitting a \emph{light} surrogate model
$\hat{g}_i$ for approximating $g_i$ around $\theta_i^*$.

ROMC proposes fitting a simple quadratic model\footnote{Any ML
  regression model may be chosen as local surrogate. The choice should
  consider the properties of the local region (i.e. size,
  smoothness). } by sampling $N$ points from the bounding box and the
labels $Y: \mathbb{R}^{N}$ are computed by evaluating $g_i$. This
additional step adds an extra effort in the training part, but
promises a significant speed-up at the inference phase. Such imbalance
is usual in the context of ML where is usual to be quite generous with
the execution time at the training phase but quite eager at the
inference phase. Fitting a local surrogate model aligns with this
requirement.
