The simplifications introduced by OMC, although beneficial from a
computational point-of-view, they suffer from some drawbacks:

\begin{itemize}
\item The whole acceptable region $\mathcal{S}_i$
  shrinks to a single point $\thetab_i^*$. This simplification adds
  significant apprimation error when then the area $\mathcal{S}_i$ is relatively big.
\item The weight $w_i$ is computed based only at the curvature at the
  point $\thetab_i^*$. Using only local information for estimating the weight can also be misleading. For example, if $g_i$ is almost flat at $\thetab_i^*$, i.e. $\text{det}(\jac_i^{*T}\jac_i^*) \rightarrow 0, then \Rightarrow w_i
  \rightarrow \infty$ dominating the posterior.
\item There is no way to solve the optimisation problem
  $\thetab_i^* = \text{argmin}_{\thetab} \: g_i(\thetab)$ when $g_i$
  is not differentiable.
\end{itemize}

\subsubsection{Sampling and computing expectation in ROMC}

ROMC resolves the failure-modes of OMC mainly by approaching the
acceptance regions $\mathcal{S}_i$ in a more sophisticated way.
Instead of collapsing the regions into single points, it approximates
them with a bounding box. Afterwards, a uniform distribution defined
on the bounding box area, is used as the \emph{proposal distribution}
for importance sampling. Defining as $q_i$ the uniform distribution
defined on the $i-th$ bounding box, weighted sampling is performed by
drawing samples from $q_i$ and assigning the weights as
in~\eqref{eq:sampling}.

\begin{gather}
  \thetab_{ij} \sim q_i \\  \label{eq:sampling}
  w_{ij} = \frac{\indicator{\region(\data)}(M_d(\thetab_{ij}, \vb_i)) p(\thetab_{ij})}{q(\thetab_{ij})}
\end{gather}

\noindent

Having defined the procedure for obtaining weighted samples, any
expectation $E_{p(\thetab|\data)}[h(\thetab)]$, can be approximated
as,

\begin{equation} \label{eq:expectation}
  E_{p(\thetab|\data)}[h(\thetab)] \approx \frac{\sum_{ij} w_{ij} h(\thetab_{ij})}{\sum_{ij} w_{ij}}
\end{equation}

The constrution of the proposal regions $q_i$ is performed in two
steps; (a) solving the deterministic optimisation problems as in OMC
and (b) constructing the bounding box regions.

\subsubsection*{Solving the deterministic optimisation problems}

For each set of nuisance variables $\vb_i, i = \{1,2,\ldots,n_1 \}$,
we search for a point $\thetab^*_i$ such that
$d(M_d(\thetab^*_i,\vb_i), \data) < \epsilon$. For notation
convenience, we name the distance function $d(M_d(\thetab,\vb_i), \data)$ as
$d_i(\thetab)$.  Obtaining this point involves solving the following
optimisation problem:

\begin{subequations}
\begin{alignat}{2}
  &\!\min_{\thetab}        &\qquad& d_i(\thetab) = d(M_d(\thetab^*,\vb_i), \data)\label{eq:optProb}\\
  &\text{subject to} & & d_i(\thetab) \leq \epsilon
\end{alignat}
\end{subequations}
%
The optimisation problem can be treated as unconstrained, accepting
the optimal point $\thetab_i^* = \text{argmin}_{\thetab} d_i(\thetab)$
only if $d_i(\thetab_i^*) < \epsilon$. The nature of the generative
model $M_r(\thetab)$, specifies the properties of the objective
function $d_i$. If $d_i$ is differentiable any gradient-based
iterative algorithm can be used for~\ref{eq:optProb}. The gradients
$\nabla_{\thetab} d_i$ can be either provided in closed form or
approximated by finite differences. Alternatively, if $d_i$ is not
differentiable, the Bayesian Optimisation
approach~\cite{Shahriari2016} provides an alternative choice. In this
case, apart from obtaining an optimal $\thetab_i^* $, a surrogate
model $\hat{d}_i$ of the distance $d_i$ is also fitted; the
approximate model can then substitute the actual model in the
following steps.

\subsubsection*{Proposal region computation}

After obtaining $\thetab_i^*$, we initially approximate the acceptance
region $\mathcal{S}_i$ by constructing a bounding box
$\mathcal{\hat{S}}_i$ around it. The bounding must the part of the
acceptance region $\mathcal{S}_i$ \emph{around} $\thetab_i^*$;
formally $\{ \thetab : d_i(\thetab) < \epsilon$,
$d(\thetab, \thetab_i^*) < M \}$. The condition
$d(\thetab, \thetab_i^*) < M$ describes that if
$\mathcal{S}_i := \{ \thetab : d_i(\thetab) < \epsilon \} $ contains
many \emph{disjoint} sets of $\thetab$ that they fulfill
$d_i(\thetab) < \epsilon$, we want the bounding box to match only the
set that contains $\thetab_i^*$. Ideally, we want the bounding box to
be as tight as possible to $\mathcal{S}_i$, to ensure high acceptance
rate in the importance sampling, and big enough for not discarding
valid parts of $\mathcal{S}_i$.

The bounding boxes are built in two steps; firstly, we define their
axes $\mathbf{v}_d$ in the $d$-dimensional space and, then, the limits
are obtained with a line-search method looking for the smallest
$\kappa: d_i(\thetab_i^* + \kappa \vb_d) \geq
\epsilon$\footnote{$-\kappa$ is used as well for the opposite
  direction along the search
  line}. Algorithm~\ref{alg:region_construction} describes the method
in-depth. After limits are found along all directions, the uniform
distribution $q_i$ is defined on the bounding box, and is used as the
proposal region for importance sampling.

\subsubsection*{Fitting a local surrogate model $\hat{g}_i$}

After the construction of the bounding box $\mathcal{\hat{S}}_i$, we
are no longer interested in $d_i(\thetab)$ outside
$\mathcal{\hat{S}}_i$. In the inference steps
(i.e. sampling~\eqref{eq:sampling}, computing
expectation~\eqref{eq:expectation}, evaluating the
posterior~\eqref{eq:approx_posterior_omc}), we need to evaluate the
distance function $d_i$ for checking if the samples obtained from the
uniform distribution $\thetab_{ij} \sim q_i$ lie inside the acceptance
region $\mathcal{S}_i$. Evaluating $d_i$ involves running the
deterministic simulator $M_d(\thetab_{ij}, \vb_i)$, which can be
inneficient when the model is big.\footnote{If Bayesian Optimization
  has been used for obtaining $\thetab_i^*$, then $\hat{d}_i$ can be
  used as an alternative.}. Hence, a \emph{light} surrogate model
$\Tilde{d}_i$ can be fit for approximating $d_i$ around $\theta_i^*$.

ROMC proposes fitting a simple quadratic model\footnote{Any ML
  regression model may be chosen as local surrogate. The choice should
  consider the properties of the local region (i.e. size,
  smoothness). } by sampling training points from the bounding box and
labeling them by evaluating $d_i$. This additional step adds an extra
effort in the training part, but promises a significant speed-up at
the inference phase. Such time-badget imbalance between the training
and the inference part is demanded in many ML contexts; it is usual to
be quite generous with the execution time at the training phase but
quite eager at the inference phase. Fitting a local surrogate model
aligns with this requirement.
