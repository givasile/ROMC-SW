\citet{Ikonomov2019} showed that considering infinitesimally small
ellipses can lead to highly overconfident posteriors. We refer the
reader to their paper for the technical details and conditions for
this issue to occur. Intuitively, it happens because the weights in
OMC are only computed from information at \(\thetab_i^*\), and using
only local information can be misleading. For example if the curvature
of \(||\Phi(\data)-\Phi(\simulator(\thetab, \vb_i))||_2\) at
\(\thetab_i^*\) is nearly flat, the curvature alone may wrongly
indicate that \(\accregioni\) is much larger than it actually is. In
our software package we implement the robust generalisation of OMC by
\citet{Ikonomov2019} that resolves this issue.

ROMC approximates the acceptance regions \(\accregioni\) and on them defines
local proposal distributions \(q_i(\thetab)\) from which the
posterior samples \(\thetab_{ij} \sim q_i\) are generated. The samples
are assigned (importance) weights \(w_{ij}\) that compensate for using
the proposal distributions \(q_i(\thetab)\) and not the prior
\(p(\thetab)\),
\begin{equation}
  w_{ij} = \frac{\indicator{\accregioni}(\thetab_{ij}) p(\thetab_{ij})}{q(\thetab_{ij})}.
  \label{eq:sampling}
\end{equation}
Given the weighted samples, any expectation
\(\Ex_{p(\thetab|\data)}[h(\thetab)]\) of some function \(h(\thetab)\), can be approximated as
\begin{equation} \label{eq:expectation}
  \Ex_{p(\thetab|\data)}[h(\thetab)] \approx \frac{\sum_{ij} w_{ij} h(\thetab_{ij})}{\sum_{ij} w_{ij}}
\end{equation}
\citet{Ikonomov2019} considered uniform distributions as proposal
distributions so that the main task is to approximate the acceptance
regions \(\accregioni\) and to represent them so that uniform sampling
is easy. The approximation of the acceptance regions contains two
compulsory and one optional step: (1) solving the optimisation
problems as in OMC, (2) constructing bounding boxes around
\(\accregioni\) and optionally, (3) refining the approximation via a
surrogate model of the distance.

\subsubsection*{Solving the deterministic optimisation problems}
For each set of nuisance variables \(\vb_i, i = \{1,2,\ldots,n_1 \}\), we
search for a point \(\thetab^*_i\) such that
\(d(\simulator(\thetab^*_i,\vb_i), \data) \le \epsilon\). For notational
convenience, we denote the distance \(d(\simulator(\thetab,\vb_i), \data)\) by
\(d_i(\thetab)\).  Obtaining \(\theta_i^*\) involves solving the following
optimisation problem:
\begin{subequations}
\begin{alignat}{2}      
  &\!\min_{\thetab}        && d_i(\thetab) \label{eq:optProb}\\
  &\text{subject to} & \quad& d_i(\thetab) \leq \epsilon
\end{alignat}
\end{subequations}
%
The optimisation problem can be treated as unconstrained, accepting
the optimal point
\(\thetab_i^* = \text{argmin}_{\thetab} d_i(\thetab)\) only if
\(d_i(\thetab_i^*) \le \epsilon\). If \(d_i(\thetab)\) is
differentiable any gradient-based optimizer can be used
for~\ref{eq:optProb}. The gradients \(\nabla_{\thetab} d_i(\thetab)\)
can be either provided in closed form or approximated by finite
differences. In case \(d_i\) is not differentiable, Bayesian
Optimisation~\citep{Shahriari2016} provides an alternative
approach. In this scenario, apart from obtaining an optimal
\(\thetab_i^* \), a surrogate model \(\hat{d}_i(\thetab)\) of the
distance function \(d_i(\thetab)\) is also automatically obtained;
\(\hat{d}_i\) can then substitute the actual distance function in
downstream steps of the algorithms, which can lead to computational
gains if running the simulator for evaluating the actual distance
\(d_i(\thetab)\) is expensive.

\subsubsection*{Estimating the acceptance regions}
The acceptance region \(\accregioni\) is approximated by a bounding
box \(\accregionihat\). Ideally, we want the bounding box to be as
tight as possible to \(\mathcal{S}_i\), to ensure high acceptance rate
in the importance sampling, but big enough for not discarding valid
parts of \(\mathcal{S}_i\). The bounding boxes are built in two
steps. First, we define their axes \(\mathbf{v}_d\) based on the
(estimated) curvature of the distance at \(\thetab_i^*\). Second, we
determine the size of the box via a one-dimensional line-search method
along each axix, see Algorithm~\ref{alg:region_construction} for the
details. After the bounding boxes construction, a uniform distribution
\(q_i\) is defined on each bounding box, and is used as the proposal
region for importance sampling.

\subsubsection*{Refining the estimate via a local surrogate model (optional)}
When computing the weight \(w_{ij}\) in \eqref{eq:sampling}, we need
to check whether the samples \(\thetab_{ij} \sim q_i\) lie inside the
acceptance region \(\accregioni\). This can be considered to be a
safety-mechanism that corrects for any inaccuracies in the
construction of \(\accregionihat\) above. However, this check involves
evaluting the distance function \(d_i(\thetab_{ij})\), which can be
expensive if the model is complex. \citet{Ikonomov2019} thus proposed
to fit a surrogate model \(\tilde{d}_i(\thetab)\) of the distance
function \(d_i(\thetab)\), on data points that lie inside
\(\accregionihat\). They used a simple quadratic model whilst other
regression models are, in principle, possible too. The advantage of
using a quadratic model is that it has ellipsoidal isocontours, which
thus naturally allowed \citet{Ikonomov2019} to replace the bounding
box approximation of \(\accregioni\) with a tigher-fitting ellipsoidal
approximation.\footnote{The difference to the infinitesimal
  ellipsoidal model in OMC is the estimation procedure: OMC uses
  information at \(\thetab_i^*\) whilst, here, information in
  \(\accregionihat\) is used, which results in a more stable fit.}

The training data for the quadratic model is obtained by sampling
\(\thetab_{ij} \sim q_i\) and accessing the distances
\(d_i(\thetab_{ij})\). The generation of the training data adds an
extra computational cost, but leads to a significant speed-up when
evaluating the weights \(w_{ij}\). Moreover, the extra cost is largely
eliminated if Bayesian Optimisation with a Gaussian process (GP)
surrogate model \(\hat{d}_i(\thetab)\) was used to obtain
\(\thetab_i^*\) in the first step. In this case, we can use
\(\hat{d}_i(\thetab)\) instead of \(d_i(\thetab)\) to generate the
training data. This essentially replaces the global GP model with a
simpler local quadratic model which is typically more robust.
