% Encoding: ISO-8859-1
@Book{Cameron+Trivedi:2013,
  author = {A. Colin Cameron and Pravin K. Trivedi},
  title = {Regression Analysis of Count Data},
  year = {2013},
  edition = {2nd},
  publisher = {Cambridge University Press},
  address = {Cambridge},
}

@Book{Chambers+Hastie:1992,
  editor = {John M. Chambers and Trevor J. Hastie},
  title = {Statistical Models in \proglang{S}},
  publisher = {Chapman \& Hall},
  year = {1992},
  address = {London},
}

@Manual{Jackman:2015,
  title = {\pkg{pscl}: Classes and Methods for \proglang{R} Developed in the Political Science Computational Laboratory, Stanford University},
  author = {Simon Jackman},
  year = {2015},
  note = {\proglang{R} package version 1.4.9},
  url = {https://CRAN.R-project.org/package=pscl},
}

@Article{Mullahy:1986,
  author = {John Mullahy},
  title = {Specification and Testing of Some Modified Count Data Models},
  year = {1986},
  journal = {Journal of Econometrics},
  volume = {33},
  number = {3},
  pages = {341--365},
  doi = {10.1016/0304-4076(86)90002-3},
}

@Book{McCullagh+Nelder:1989,
  author = {Peter McCullagh and John A. Nelder},
  title = {Generalized Linear Models},
  edition = {2nd},
  year = {1989},
  publisher = {Chapman \& Hall},
  address = {London},
  doi = {10.1007/978-1-4899-3242-6},
}

@Manual{R,
  title = {\proglang{R}: {A} Language and Environment for Statistical Computing},
  author = {{\proglang{R} Core Team}},
  organization = {\proglang{R} Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2017},
  url = {https://www.R-project.org/},
}

@Article{Stasinopoulos+Rigby:2007,
  author = {D. Mikis Stasinopoulos and Robert A. Rigby},
  title = {Generalized Additive Models for Location Scale and Shape ({GAMLSS}) in \proglang{R}},
  journal = {Journal of Statistical Software},
  year = {2007},
  volume = {23},
  number = {7},
  pages = {1--46},
  doi = {10.18637/jss.v023.i07},
}

@Book{Venables+Ripley:2002,
  author = {William N. Venables and Brian D. Ripley},
  title = {Modern Applied Statistics with \proglang{S}},
  edition = {4th},
  year = {2002},
  pages = {495},
  publisher = {Springer-Verlag},
  address = {New York},
  doi = {10.1007/978-0-387-21706-2},
}

@Book{Wood:2006,
  author = {Simon N. Wood},
  title = {Generalized Additive Models: An Introduction with \proglang{R}},
  year = {2006},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton},
}

@Article{Yee:2009,
  author = {Thomas W. Yee},
  title = {The \pkg{VGAM} Package for Categorical Data Analysis},
  journal = {Journal of Statistical Software},
  year = {2010},
  volume = {32},
  number = {10},
  pages = {1--34},
  doi = {10.18637/jss.v032.i10},
}

@Article{Zeileis+Kleiber+Jackman:2008,
  author = {Achim Zeileis and Christian Kleiber and Simon Jackman},
  title = {Regression Models for Count Data in \proglang{R}},
  journal = {Journal of Statistical Software},
  year = {2008},
  volume = {27},
  number = {8},
  pages = {1--25},
  doi = {10.18637/jss.v027.i08},
}


%% Vasilis bibliography
@article{Tanaka2006,
abstract = {Tuberculosis can be studied at the population level by genotyping strains of Mycobacterium tuberculosis isolated from patients. We use an approximate Bayesian computational method in combination with a stochastic model of tuberculosis transmission and mutation of a molecular marker to estimate the net transmission rate, the doubling time, and the reproductive value of the pathogen. This method is applied to a published data set from San Francisco of tuberculosis genotypes based on the marker IS6110. The mutation rate of this marker has previously been studied, and we use those estimates to form a prior distribution of mutation rates in the inference procedure. The posterior point estimates of the key parameters of interest for these data are as follows: net transmission rate, 0.69/year [95{\%} credibility interval (C.I.) 0.38, 1.08]; doubling time, 1.08 years (95{\%} C.I. 0.64, 1.82); and reproductive value 3.4 (95{\%} C.I. 1.4, 79.7). These figures suggest a rapidly spreading epidemic, consistent with observations of the resurgence of tuberculosis in the United States in the 1980s and 1990s. Copyright {\textcopyright} 2006 by the Genetics Society of America.},
author = {Tanaka, Mark M. and Francis, Andrew R. and Luciani, Fabio and Sisson, S. A.},
doi = {10.1534/genetics.106.055574},
issn = {00166731},
journal = {Genetics},
mendeley-groups = {Edinburgh{\_}thesis/bibliography},
pmid = {16624908},
title = {{Using approximate bayesian computation to estimate tuberculosis transmission parameters from genotype data}},
year = {2006}
}

@inproceedings{Chen2019,
abstract = {Approximate Bayesian computation (ABC) is a set of techniques for Bayesian inference when the likelihood is intractable but sampling from the model is possible. This work presents a simple yet effective ABC algorithm based on the combination of two classical ABC approaches â€” regression ABC and sequential ABC. The key idea is that rather than learning the posterior directly, we first target another auxiliary distribution that can be learned accurately by existing methods, through which we then subsequently learn the desired posterior with the help of a Gaussian copula. During this process, the complexity of the model changes adaptively according to the data at hand. Experiments on a synthetic dataset as well as three real-world inference tasks demonstrates that the proposed method is fast, accurate, and easy to use.},
author = {Chen, Yanzhi and Gutmann, Michael U},
booktitle = {Proceedings of Machine Learning Research},
file = {:home/ntipakos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Gutmann - 2019 - Adaptive Gaussian Copula ABC.pdf:pdf},
mendeley-groups = {Edinburgh{\_}thesis/Theory,Edinburgh{\_}thesis,Edinburgh{\_}thesis/bibliography},
pages = {1584--1592},
title = {{Adaptive Gaussian Copula ABC}},
url = {http://proceedings.mlr.press/v89/chen19d.html},
volume = {89},
year = {2019}
}
@inproceedings{Meeds2015,
abstract = {We describe an embarrassingly parallel, anytime Monte Carlo method for likelihood-free models. The algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic. For each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data. After reweighing these samples using the prior and the Jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a Monte Carlo estimate of the posterior distribution. The procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample). The procedure is validated on six experiments.},
archivePrefix = {arXiv},
arxivId = {1506.03693},
author = {Meeds, Edward and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1506.03693},
bfile = {:home/ntipakos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meeds, Welling - 2015 - Optimization Monte Carlo Efficient and embarrassingly parallel likelihood-free inference.pdf:pdf},
issn = {10495258},
mendeley-groups = {Edinburgh{\_}thesis/Theory,Edinburgh{\_}thesis/bibliography},
title = {{Optimization Monte Carlo: Efficient and embarrassingly parallel likelihood-free inference}},
year = {2015}
}

@article{Ikonomov2019,
abstract = {This paper is on Bayesian inference for parametric statistical models that are implicitly defined by a stochastic simulator which specifies how data is generated. While exact sampling is possible, evaluating the likelihood function is typically prohibitively expensive. Approximate Bayesian Computation (ABC) is a framework to perform approximate inference in such situations. While basic ABC algorithms are widely applicable, they are notoriously slow and much research has focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has recently been proposed as an efficient and embarrassingly parallel method that leverages optimisation to accelerate the inference. In this paper, we demonstrate a previously unrecognised important failure mode of OMC: It generates strongly overconfident approximations by collapsing regions of similar or near-constant posterior density into a single point. We propose an efficient, robust generalisation of OMC that corrects this. It makes fewer assumptions, retains the main benefits of OMC, and can be performed either as part of OMC or entirely as post-processing. We demonstrate the effectiveness of the proposed Robust OMC on toy examples and tasks in inverse-graphics where we perform Bayesian inference with a complex image renderer.},
archivePrefix = {arXiv},
arxivId = {1904.00670},
author = {Ikonomov, Borislav and Gutmann, Michael U.},
journal = {AISTATS},
eprint = {1904.00670},
title = {{Robust Optimisation Monte Carlo}},
url = {http://arxiv.org/abs/1904.00670},
year = {2020}
}
@article{Lintusaari2017,
abstract = {Bayesian inference plays an important role in phylogenetics, evolutionary biology, and in many other branches of science. It provides a principled framework for dealing with uncertainty and quantifying how it changes in the light of new evidence. For many complex models and inference problems, however, only approximate quantitative answers are obtainable. Approximate Bayesian computation (ABC) refers to a family of algorithms for approximate inference that makes a minimal set of assumptions by only requiring that sampling from a model is possible.We explain here the fundamentals of ABC, review the classical algorithms, and highlight recent developments.},
author = {Lintusaari, Jarno and Gutmann, Michael U. and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
doi = {10.1093/sysbio/syw077},
file = {:home/ntipakos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lintusaari et al. - 2017 - Fundamentals and recent developments in approximate Bayesian computation.pdf:pdf},
issn = {1076836X},
journal = {Systematic Biology},
keywords = {ABC,Approximate Bayesian computation,Bayesian inference,Likelihood-free inference,Phylogenetics,Simulator-based models,Stochastic simulation models,Treebased models},
mendeley-groups = {Edinburgh{\_}thesis/Theory,Edinburgh{\_}thesis,Edinburgh{\_}thesis/bibliography},
number = {1},
pages = {e66--e82},
title = {{Fundamentals and recent developments in approximate Bayesian computation}},
volume = {66},
year = {2017}
}


@misc{1708.00707,
Author = {Jarno Lintusaari and Henri Vuollekoski and Antti KangasrÃ¤Ã¤siÃ¶ and Kusti SkytÃ©n and Marko JÃ¤rvenpÃ¤Ã¤ and Pekka Marttinen and Michael Gutmann and Aki Vehtari and Jukka Corander and Samuel Kaski},
Title = {ELFI: Engine for Likelihood Free Inference},
Year = {2018},
Eprint = {arXiv:1708.00707},
}

@misc{Shahriari2016,
abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {De Freitas}, Nando},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2015.2494218},
issn = {15582256},
keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
mendeley-groups = {Edinburgh{\_}thesis/bibliography},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
year = {2016}
}

@article{Marin2012,
abstract = {Approximate Bayesian Computation (ABC) methods, also known as likelihood-free techniques, have appeared in the past ten years as the most satisfactory approach to intractable likelihood problems, first in genetics then in a broader spectrum of applications. However, these methods suffer to some degree from calibration difficulties that make them rather volatile in their implementation and thus render them suspicious to the users of more traditional Monte Carlo methods. In this survey, we study the various improvements and extensions brought on the original ABC algorithm in recent years. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
archivePrefix = {arXiv},
arxivId = {1101.0955},
author = {Marin, Jean Michel and Pudlo, Pierre and Robert, Christian P. and Ryder, Robin J.},
doi = {10.1007/s11222-011-9288-2},
eprint = {1101.0955},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {ABC methodology,Bayesian model choice,Bayesian statistics,DIYABC,Likelihood-free methods},
mendeley-groups = {Edinburgh{\_}thesis/bibliography},
title = {{Approximate Bayesian computational methods}},
year = {2012}
}

@ARTICLE{2020SciPy-NMeth,
       author = {{Virtanen}, Pauli and {Gommers}, Ralf and {Oliphant},
         Travis E. and {Haberland}, Matt and {Reddy}, Tyler and
         {Cournapeau}, David and {Burovski}, Evgeni and {Peterson}, Pearu
         and {Weckesser}, Warren and {Bright}, Jonathan and {van der Walt},
         St{\'e}fan J.  and {Brett}, Matthew and {Wilson}, Joshua and
         {Jarrod Millman}, K.  and {Mayorov}, Nikolay and {Nelson}, Andrew
         R.~J. and {Jones}, Eric and {Kern}, Robert and {Larson}, Eric and
         {Carey}, CJ and {Polat}, {\.I}lhan and {Feng}, Yu and {Moore},
         Eric W. and {Vand erPlas}, Jake and {Laxalde}, Denis and
         {Perktold}, Josef and {Cimrman}, Robert and {Henriksen}, Ian and
         {Quintero}, E.~A. and {Harris}, Charles R and {Archibald}, Anne M.
         and {Ribeiro}, Ant{\^o}nio H. and {Pedregosa}, Fabian and
         {van Mulbregt}, Paul and {Contributors}, SciPy 1. 0},
        title = "{SciPy 1.0: Fundamental Algorithms for Scientific
                  Computing in Python}",
      journal = {Nature Methods},
      year = "2020",
      volume={17},
      pages={261--272},
      adsurl = {https://rdcu.be/b08Wh},
      doi = {https://doi.org/10.1038/s41592-019-0686-2},
}


@Misc{gpy2014,
  author =   {{GPy}},
  title =    {{GPy}: A Gaussian process framework in python},
  howpublished = {\url{http://github.com/SheffieldML/GPy}},
  year = {since 2012}
}


@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@conference{Kluyver:2016aa,
	Author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing},
	Booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
	Editor = {F. Loizides and B. Schmidt},
	Organization = {IOS Press},
	Pages = {87 - 90},
	Title = {Jupyter Notebooks -- a publishing format for reproducible computational workflows},
	Year = {2016}}


@inbook{Bisong2019,
abstract = {Google Colaboratory more commonly referred to as ``Google Colab'' or just simply ``Colab'' is a research project for prototyping machine learning models on powerful hardware options such as GPUs and TPUs. It provides a serverless Jupyter notebook environment for interactive development. Google Colab is free to use like other G Suite products.},
address = {Berkeley, CA},
author = {Bisong, Ekaba},
booktitle = {Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners},
doi = {10.1007/978-1-4842-4470-8_7},
isbn = {978-1-4842-4470-8},
mendeley-groups = {Edinburgh{\_}thesis/bibliography},
pages = {59--64},
publisher = {Apress},
title = {{Google Colaboratory}},
url = {https://doi.org/10.1007/978-1-4842-4470-8{\_}7},
year = {2019}
}

@article{Sudman1967,
author = {Sudman, Seymour},
doi = {10.1086/224359},
issn = {0002-9602},
journal = {American Journal of Sociology},
mendeley-groups = {Edinburgh{\_}thesis/bibliography},
title = {{ Survey Sampling. Leslie Kish }},
year = {1967}
}

@Book{Sisson2018,
  editor    = {Sisson, S.A. and Fan, Y and Beaumont, M.A.},
  publisher = {Chapman and Hall/CRC Press},
  title     = {Handbook of Approximate Bayesian Computation.},
  year      = {2018},
  file      = {Sisson2018.pdf:home/mgutmann/Library/Papers/Sisson2018.pdf:PDF},
  journal   = {arXiv:1802.09720},
  owner     = {mgutmann},
  timestamp = {2018.09.13},
}

@Article{Gutmann2016,
  author    = {Michael U. Gutmann and Jukka Corander},
  journal   = {Journal of Machine Learning Research},
  title     = {Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models},
  year      = {2016},
  note      = {http://arxiv.org/abs/1501.03291},
  number    = {125},
  pages     = {1--47},
  volume    = {17},
  file      = {Gutmann2016.pdf:gutmann_publ/Gutmann2016.pdf:PDF;arxiv:gutmann_publ/Gutmann2015a.pdf:PDF},
  groups    = {Journal and Proceedings},
  owner     = {mgutmann},
  timestamp = {2021.07.29},
  url       = {http://jmlr.org/papers/v17/15-017.html},
}

@Article{Cranmer2020,
  author       = {Cranmer, K. and Brehmer, J. and Louppe, G.},
  journal      = {Proceedings of the National Academy of Sciences},
  title        = {The frontier of simulation-based inference},
  year         = {2020},
  abstract     = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.},
  elocation-id = {201912789},
  owner        = {mgutmann},
  publisher    = {National Academy of Sciences},
  timestamp    = {2021.07.29},
}

@Article{Blum2010,
  author    = {Blum, Michael and Francois, Olivier},
  journal   = {Statistics and Computing},
  title     = {{N}on-linear regression models for {A}pproximate {B}ayesian {C}omputation},
  year      = {2010},
  number    = {1},
  pages     = {63--73},
  volume    = {20},
  abstract  = {Approximate Bayesian inference on the basis of summary statistics is well-suited to complex problems for which the likelihood is either mathematically or computationally intractable. However the methods that use rejection suffer from the curse of dimensionality when the number of summary statistics is increased. Here we propose a machine-learning approach to the estimation of the posterior density by introducing two innovations. The new method fits a nonlinear conditional heteroscedastic regression of the parameter on the summary statistics, and then adaptively improves estimation using importance sampling. The new algorithm is compared to the state-of-the-art approximate Bayesian methods, and achieves considerable reduction of the computational burden in two examples of inference in statistical genetics and in a queueing model.},
  file      = {Blum2010.pdf:Blum2010.pdf:PDF},
  groups    = {ABC},
  issn      = {0960-3174},
  keywords  = {Mathematics and Statistics},
  owner     = {gutmann},
  publisher = {Springer Netherlands},
  timestamp = {2012.05.19},
  url       = {http://dx.doi.org/10.1007/s11222-009-9116-0},
}

@Inproceedings{Papamakarios2016,
  author    = {Papamakarios, George and Murray, Iain},
  booktitle = {Advances in Neural Information Processing Systems 29},
  title     = {Fast epsilon-free Inference of Simulation Models with {B}ayesian Conditional Density Estimation},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {1028--1036},
  publisher = {Curran Associates, Inc.},
  file      = {Papamakarios2016.pdf:home/mgutmann/Library/Papers/Papamakarios2016.pdf:PDF},
  owner     = {mgutmann},
  timestamp = {2021.07.29},
  url       = {http://papers.nips.cc/paper/6084-fast-free-inference-of-simulation-models-with-bayesian-conditional-density-estimation},
}

@Inproceedings{Papamakarios2019,
  author        = {Papamakarios, George and Sterratt, David and Murray, Iain},
  booktitle     = {Proceedings of Machine Learning Research},
  title         = {Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows},
  year          = {2019},
  address       = {Proceedings of Machine Learning Research},
  editor        = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  pages         = {837--848},
  publisher     = {PMLR},
  volume        = {89},
  __markedentry = {[mgutmann:]},
  abstract      = {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.},
  journal       = {Proceedings of Machine Learning Research},
  owner         = {mgutmann},
  timestamp     = {2019.09.23},
}

@Article{Wood2010,
  author    = {Wood, Simon N.},
  journal   = {Nature},
  title     = {{S}tatistical inference for noisy nonlinear ecological dynamic systems},
  year      = {2010},
  month     = aug,
  number    = {7310},
  pages     = {1102--1104},
  volume    = {466},
  comment   = {10.1038/nature09319},
  file      = {Wood2010.pdf:Wood2010.pdf:PDF;Supplementary material:Wood2010_suppl.pdf:PDF},
  groups    = {Interesting_papers, ABC, Indirect_inference},
  issn      = {0028-0836},
  keywords  = {Indirect inference},
  owner     = {gutmann},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2011.08.17},
  url       = {http://dx.doi.org/10.1038/nature09319},
}

@Article{Thomas2016,
  author    = {Thomas, O. and Dutta, R. and Corander, J. and Kaski, S. and Gutmann, M.U.},
  journal   = {arXiv:1611.10242},
  title     = {Likelihood-Free Inference by Ratio Estimation},
  year      = {2016},
  abstract  = {We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of "closeness" is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on toy problems and use it to perform inference for stochastic nonlinear dynamical systems.},
  arxiv     = {https://arxiv.org/abs/1611.10242},
  file      = {:Dutta2016_arxiv.pdf:PDF},
  keywords  = {approximate Bayesian computation, synthetic likelihood, summary statistics selection, density-ratio estimation; logistic regression; probabilistic classification, stochastic dynamical systems},
  owner     = {mgutmann},
  timestamp = {2017.04.27},
}

@Article{Thomas2020,
  author    = {Thomas, O. and Dutta, R. and Corander, J. and Kaski, S. and Gutmann, M. U.},
  journal   = {Bayesian Analysis},
  title     = {Likelihood-Free Inference by Ratio Estimation},
  year      = {2020},
  number    = {advance publication},
  abstract  = {We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of "closeness" is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on toy problems and use it to perform inference for stochastic nonlinear dynamical systems.},
  arxiv     = {https://arxiv.org/abs/1611.10242},
  doi       = {https://doi.org/10.1214/20-BA1238},
  file      = {:Thomas2020.pdf:PDF},
  keywords  = {approximate Bayesian computation, synthetic likelihood, summary statistics selection, density-ratio estimation; logistic regression; probabilistic classification, stochastic dynamical systems},
  owner     = {mgutmann},
  timestamp = {2017.04.27},
  url       = {https://projecteuclid.org/euclid.ba/1599876022},
}

@Inproceedings{Hermans2020,
  author    = {Hermans, J. and Begy, V. and Louppe, G.},
  booktitle = {Proceedings of the thirty-seventh International Conference on Machine Learning (ICML)},
  title     = {Likelihood-free {MCMC} with Amortized Approximate Ratio Estimators},
  year      = {2020},
  owner     = {mgutmann},
  timestamp = {2020.06.30},
}

@Comment{jabref-meta: databaseType:bibtex;}

@INCOLLECTION{Forneron2016,
title = {A Likelihood-Free Reverse Sampler of the Posterior Distribution},
author = {Forneron, Jean-Jacques and Ng, Serena},
year = {2016},
pages = {389-415},
booktitle = {Essays in Honor of Aman Ullah},
volume = {36},
publisher = {Emerald Publishing Ltd},
abstract = {Abstract This paper considers properties of an optimization-based sampler for targeting the posterior distribution when the likelihood is intractable. It uses auxiliary statistics to summarize information in the data and does not directly evaluate the likelihood associated with the specified parametric model. Our reverse sampler approximates the desired posterior distribution by first solving a sequence of simulated minimum distance problems. The solutions are then reweighted by an importance ratio that depends on the prior and the volume of the Jacobian matrix. By a change of variable argument, the output consists of draws from the desired posterior distribution. Optimization always results in acceptable draws. Hence, when the minimum distance problem is not too difficult to solve, combining importance sampling with optimization can be much faster than the method of Approximate Bayesian Computation that by-passes optimization.},
keywords = {Approximate Bayesian Computation; indirect inference; importance sampling; C22; C23},
url = {https://EconPapers.repec.org/RePEc:eme:aecozz:s0731-905320160000036020}
}

